[
["index.html", "Applied Environmental Statitics Frontmatter Typography Interactive Content Dedication", " Applied Environmental Statitics Rodney J. Dyer Frontmatter This textbook was designed in support of the basic graduate level Applied Environmental Statistics course for the Center for Environmental Studies at Virginia Commonwealth University. T Build Date: Fri Mar 31 10:16:40 2017 The content of this text is modified, added to, and changed continually in support of this class. You are welcome to use the content of this book in its present form and you can provide feedback, comments, or suggestions for additions by contacting me at rjdyer@vcu.edu. This work will continue to be hosted online and continually updated as new approaches and applications become available. © 2017 by R.J. Dyer. This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Under this license, authorized individuals may copy, distribute, display and perform the work and make derivative works and remixes based on this text only if they give the original author credit (attribution). You are also free to distribute derivative works only under a license identical (“not more restrictive”) to the license that governs this original work. Dr. Rodney Dyer is a Professor and Director for the Center for Environmental Studies at Virginia Commonwealth University in Richmond, Virginia, USA. His research focuses on genetic connectivity and structure and how the environment influences both. More information on his research can be found at http://dyerlab.org. Typography This text is designed primarily as an electronic publication (ePub) and has dynamical content included within. If you are reading a static copy (as PDF or printed text), you are not getting the full experience of what this book has been designed to deliver. Much of the text is devoted to quantitative analysis of data in R, and as such I’ve typeset the R components differently from the flowing text. Text intended to be input into R is typeset as a fixed width font and colored using the default color scheme found in RStudio (http://rstudio.org). Here are two input examples. value &lt;- 20 name &lt;- &quot;Perdedor&quot; This text is amenable to copy-and-paste action so you can perform the same calculations on your computer as are done in the text. When R returns an answer to an analysis or prints the contents of a variable out, the results are also typeset in fixed width font but each line is prefixed by two hash marks. rnorm(10) ## [1] -0.43011099 -1.92866484 0.26217028 -0.10422355 0.01515216 ## [6] 0.52679238 -0.07619819 0.75599991 0.41916929 -0.96248913 The lines with hashes are not something you are going to copy-and-paste into your R session, it is what R is giving you. Inline code (e.g., code inserted into a sentence in a descriptive context such as discussing the rnorm() function in the previous example) is typeset similarly though not necessarily intended to be cut-and-pasted into your session. Throughout the ePub, there are also dynamical content. Some of this content may be condensed text put into a scrolling text-box. Here the notion is to provide direct access to information without unnecessarily adding length to the overall text. Here is an example, showing the documentation associated with the R help.start() function within a scrolling text box. help.start {utils} R Documentation Hypertext Documentation Description Start the hypertext (currently HTML) version of R's online documentation. Usage help.start(update = FALSE, gui = \"irrelevant\", browser = getOption(\"browser\"), remote = NULL) Arguments update - logical: should this attempt to update the package index to reflect the currently available packages. (Not attempted if remote is non-NULL.) gui - just for compatibility with S-PLUS. browser - the name of the program to be used as hypertext browser. It should be in the PATH, or a full path specified. Alternatively, it can be an R function which will be called with a URL as its only argument. This option is normally unset on Windows, when the file-association mechanism will be used. remote - A character string giving a valid URL for the ‘R_HOME' directory on a remote location. Details Unless remote is specified this requires the HTTP server to be available (it will be started if possible: see startDynamicHelp). One of the links on the index page is the HTML package index, ‘R.home(\"docs\")/html/packages.html', which can be remade by make.packages.html(). For local operation, the HTTP server will remake a temporary version of this list when the link is first clicked, and each time thereafter check if updating is needed (if .libPaths has changed or any of the directories has been changed). This can be slow, and using update = TRUE will ensure that the packages list is updated before launching the index page. Argument remote can be used to point to HTML help published by another R installation: it will typically only show packages from the main library of that installation. See Also help() for on- and off-line help in other formats. browseURL for how the help file is displayed. RSiteSearch to access an on-line search of R resources. Examples help.start() ## Not run: ## the 'remote' arg can be tested by help.start(remote = paste0(\"file://\", R.home())) ## End(Not run) In addition to shoving static content into scrolling windows, longer R scripts will also be inserted into these widgets so that space can be saved while preserving syntax highlighting and code format. Interactive Content Where possible, I have included interactive content in the text. Examples include dynamical plots such as embedding google map objects in the page, network structure that can be manipulated in the browser, and widgets that show how a process influences popualtion genetic features by allowing you to direclty manipulate the parameters in the model. Dedication I would like to dedicate this text to the first person who showed me the beauty of statistical inferences, Dr. Richard Fonda, who guided this misguided undergrad at Western Washington University. "],
["the-r-ecosystem.html", "1 The R Ecosystem 1.1 Getting R Configured 1.2 Libraries Used 1.3 The RStudio Environment", " 1 The R Ecosystem R is an open-source, community driven platform available for a wide variety of computer operating systems and is becoming a de facto standard in modern biological analyses. It is not a point-and-click interface, rather it is a rich analytical ecosystem that will make your research and scientific life more pleasurable. 1.1 Getting R Configured The grammar of the R language was derived from another system called S-Plus. S-Plus was a proprietary analysis platform developed by AT&amp;T and licenses were sold for its use, mostly in industry and education. Given the closed nature of the S-Plus platform, R was developed with the goal of creating an interpreter that could read grammar similar to S-Plus but be available to the larger research community. The use of R has increased dramatically due to its open nature and the ability of people to share code solutions with relatively little barriers. The main repository for R is located at the CRAN Repository, which is where you can download the latest version. It is in your best interests to make sure you update the underlying R system, changes are made continually (perhaps despite the outward appearance of the website). The current version of this book uses version 3.2. To get the correct version, open the page and there should be a link at the top for your particular computing platform. Download the appropriate version and install it following the instructions appropriate for your computer. 1.1.1 Packages The base R system comes with enough functionality to get you going. By default, there is only a limited amount of functionality in R, which is a great thing. You only load and use the packages that you intend to use. There are just too many packages to have them all loaded into memory at all times and there is such a broad range of packages, it is not likely you’d need more than a small fraction of the packages during the course of all your analyses. Once you have a package, you can tell R that you intend to use it by either library(package_name) or require(package_name) They are approximately equivalent, differing in only that the second one actually returns a TRUE or FALSE indicating the presence of that library on you machine. If you do not want to be mocked by other users of R, I would recommend the library version—there are situations where require will not do what you think you want it to do (even though it is a verb and should probably be the correct one to use grammatically). There are, at present, a few different places you can get packages. The packages can either be downloaded from these online repositories and installed into R or you can install them from within R itself. Again, I’ll prefer the latter as it is a bit more straightforward. 1.1.2 CRAN The main repository for packages is hosted by the r-project page itself. There are packages with solutions to analyses ranging from Approximate Bayesian Computation to Zhang &amp; Pilon’s approaches to characterizing climatic trends. The list of these packages is large and ever growing. It can be found on CRAN under the packages menu. To install a package from this repository, you use the function install.packages(&quot;thePackageName&quot;) You can see that R went to the CRAN mirror site (I use the rstudio one), downloaded the package, look for particular dependencies that that package may have and download them as well for you. It should install these packages for you and give you an affirmative message indicating it had done so. At times, there are some packages that are not available in binary form for all computer systems (the rgdal package is one that comes to mind for which we will provide a work around later) and the packages need to be compiled. This means that you need to have some additional tools and/or libraries on your machine to take the code, compile it, and link it together to make the package. In these cases, the internet and the documentation that the developer provide are key to your sanity. For packages on CRAN, you can also keep them updated to the latest version using the command: update.packages(ask=FALSE) I recommend using the ask=FALSE option on this one as it can be very tedious to respond to a “Y/N” question for each library you may need to update (there could be hundreds). 1.1.3 GitHub There are an increasing number of projects that are being developed either in the open source community or shared among a set of developers. These projects are often hosted on http://www.github.com where you can get access to the latest code updates. The packages that I develop are hosted on Github at (http://github.com/dyerlab) and only pushed to CRAN when I make major changes. To install packages from Github you need to install the devtools library from CRAN first install.packages(&quot;devtools&quot;) library(devtools) Then you need to use the devtools function install_github() to go grab it. To do so you need two separate pieces of information, the name of the developer who is creating the repository and the name of the repository it is contained within. For the gstudio package, the develop is dyerlab and the repository name is gstudio. If you are comfortable with git, you can also check out certain branches of development if you like with optional arguments to the install_github() function. Here is how you would install both the gstudio and popgraph libraries I use in this book. install_github(&quot;dyerlab/gstudio&quot;) install_github(&quot;dyerlab/popgraph&quot;) If you are starting to work with R and intend to compile packages, there are some tools that you will need to have on your machine. For Windows machines, there is an rtools download EXE that you can get. On OSX, you need to download the developers tools from Apple (available on the Application Store for free). In either case, the developer (if they care about their code being used by others) should provide sufficient documentation. If you are working on Windows, I do not know how that system (and never have used it for any prolonged period of time) so you’ll have to look to the internet on where to find compilers and other developer tools. In a way similar to that for the CRAN packages, you can update existing github packages using: library(devtools) update_packages() It will ask you if you really want to do this (which has three options: ‘Yup’, ‘No’, and ‘I forget’) and then proceed. 1.1.4 Bioconductor The last common location to find packages for R is on the Bioconductor site, a collection of software for bioinformatic analyses. To install from the bioconductor site you need to download their own installer as: source(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite() And then to install packages you use biocLite(c(&quot;GenomicFeatures&quot;, &quot;AnnotationDbi&quot;)) There are no libraries used in this text from bioconductor as this text but if you shoudl do those kinds of analyses, it will be helpful to see what they have available. 1.1.5 Troublesome Packages Some packages provide a unique set of problems for getting them onto your computer. There are a variety of reasons for this and Google is your friend (though it would be easier if R was named something more unique than the 18th letter of the alphabet…). 1.1.5.1 RGDAL &amp; RGEOS Every time I upgrade in any significant way, two R libraries seem to raise their ugly heads and scream like a spoiled child—rgdal and rgeos. Why do these two have to be SOOOO much of a pain? Why can’t we have a auto build of a binary with all the options in it for OSX? Who knows? I always feel like I get the fuzzy end of the lollipop with these two. Here is my latest approach for getting them going. First you have to make sure you have the latest GDAL libraries. I used to get mine from Kyngchaos, just download the framework, install it, and then do some kind of long R CMD INSTALL dance, which seems to no longer work for me. I also tried installing from Ripley’s repository and found that: It was a version older than the one I already had on my machine, and You can’t install from that repository, there is a malformed header and the install.packages() function just barfs. Time to try something new. I typically stay away from the various installer frameworks out there on OSX to keep everything in Frameworks. But this time, I used MacPorts. You can find the latest version here. Here is how I got it to help me out. Download XCode from Apple, it is both free and has a lot of tools that make your life as a programmer easier. It is a big package and you’ll have to install the command line developer tools as well. You will be prompted on how to do this. Downloaded the version of macports for your OS, I’m currently on 10.11 and installed it with little problems. It takes a bit of time at the end of the installation because it is downloading a lot of information. Be patient. In the terminal, I updated it sudo ports -v selfupdate and again be patient, it is going to grab a lot of stuff from the internet. I then used it to install gdal as a unix library (rather than as a framework so it won’t be located in /Library/Frameworks) by sudo ports install gdal. There were a lot of dependencies for this one so it took a while. I then had R install rgdal as install.packages( rgdal, type=&quot;source&quot;) Worked like a charm. 1.2 Libraries Used This work requires several libraries that you may need to get from either CRAN or GitHub. The following if you run the following code, you should be up-to-date on the necessary packages used throughout this text. Table 1.1: R packages used in the examples shown in this book. Name Title DescTools Tools for Descriptive Statistics devtools Tools to Make Developing R Packages Easier GGally Extension to ‘ggplot2’ ggplot2 Create Elegant Data Visualisations Using the Grammar of Graphics gstudio Tools Related to the Spatial Analysis of Genetic Marker Data MASS Support Functions and Datasets for Venables and Ripley’s MASS 1.3 The RStudio Environment By itself, R can be used in the terminal or as the basic interface the installer provides. While both of these methods are sufficient for working in R, they are less than optimal (IMHO). I believe one of the most important tools you can use is a good IDE as it helps you organize and work with your data in ways that focus more on the outcome and less on the implementation. I’ve spent a lot of time in both Emacs and Vim and while they may be tools for the geek elite, RStudio has made me much more productive in terms of output per unit time. The RStudio IDE can be can be downloaded directly from (http://rstudio.org) and comes in varieties for desktops and servers. The packages are easy to install and contain all the instructions you need. If you run your own server, you can install it and do your analyses via a web interface that looks identical to the desktop client (in fact it is more similar than you perhaps know). The interface has four panes, two of which (the source code editor and the terminal output) you will work with the most often. The other panes have information on the history of your project, plot output, packages, help, command history, a list of all variables, and a file browser (in the image above, I have minimized the pane with the “Environment” and “History” tabs on it to make more room for the code editor). You can type in R commands in the terminal window and receive responses directly, just as normal. However, there are several alternative ways you can extract use R including: Code can be entered into a script file (ending in *.R) and run them directly by submitting that file to the R interpreter. This kind of file only has code (and comments) in it. A Markdown (*.Rmd) or LaTeX (*.Rwd) file that mixes code and non-code documenation together. This text is acutally written in RMarkdown and mixes together the word-processing parts and the analytical code in a single file. This is a very important and useful approach. If you are a serious user of R, you can use LaTeX or RMarkdown to make documents, slides, presentations, posters, etc. It is a versatile tool and one worth looking into, especially as it pertains to reproducible research (something we all need to strive for). "],
["data-types.html", "2 Data Types 2.1 Numeric Data Types 2.2 Characters 2.3 Factors 2.4 Logical Types", " 2 Data Types The data we work with comes in many forms—integers, stratum, categories, genotypes, etc.—all of which we need to be able to work with in our analyses. In this chapter, the basic data types we will commonly use in population genetic analyses. This section covers some of the basic types of data we will use in R. These include numbers, character, factors, and logical data types. We will also introduce the locus object from the gstudio library and see how it is just another data type that we can manipulate in R. The very first hurdle you need to get over is the oddness in the way in which R assigns values to variables. variable &lt;- value Yes that is a less-than and dash character. This is the assignment operator that historically has been used and it is the one that I will stick with. In some cases you can use the ‘=’ to assign variables instead but then it takes away the R-ness of R itself. For decision making, the equality operator (e.g., is this equal to that) is the double equals sign ‘==’. We will get into that below where we talk about logical types and later in decision making. If you are unaware of what type a particular variable may be, you can always use the type() function and R will tell you. class( variable ) R also has a pretty good help system built into itself. You can get help for any function by typing a question mark in front of the function name. This is a particularly awesome features because at the end of the help file, there is often examples of its usage, which are priceless. Here is the documentation for the ‘help’ function as given by: ?help There are also package vignettes available (for most packages you download) that provide additional information on the routines, data sets, and other items included in these packages. You can get a list of vignettes currently installed on your machine by: vignette() and vignettes for a particular package by passing the package name as an argument to the function itself. 2.1 Numeric Data Types The quantitative measurements we make are often numeric, in that they can be represented as as a number with a decimal component (think weight, height, latitude, soil moisture, ear wax viscosity, etc.). The most basic type of data in R, is the numeric type and represents both integers and floating point numbers (n.b., there is a strict integer data type but it is often only needed when interfacing with other C libraries and can for what we are doing be disregarded). Assigning a value to a variable is easy x &lt;- 3 x ## [1] 3 By default, R automatically outputs whole numbers numbers within decimal values appropriately. y &lt;- 22/7 y ## [1] 3.142857 If there is a mix of whole numbers and numbers with decimals together in a container such as c(x,y) ## [1] 3.000000 3.142857 then both are shown with decimals. The c() part here is a function that combines several data objects together into a vector and is very useful. In fact, the use of vectors are are central to working in R and functions almost all the functions we use on individual variables can also be applied to vectors. A word of caution should be made about numeric data types on any computer. Consider the following example. x &lt;- .3 / 3 x ## [1] 0.1 which is exactly what we’d expect. However, the way in which computers store decimal numbers plays off our notion of significant digits pretty well. Look what happens when I print out x but carry out the number of decimal places. print(x, digits=20) ## [1] 0.099999999999999991673 Not quite 0.1 is it? Not that far away from it but not exact. That is a general problem, not one that R has any more claim to than any other language and/or implementation. Does this matter much, probably not in the realm of the kinds of things we do in population genetics, it is just something that you should be aware of. You can make random sets of numeric data by using using functions describing various distributions. For example, some random numbers from the normal distribution are: rnorm(10) ## [1] 0.9679307 1.2577435 -0.4562789 1.2420216 -0.5784370 0.4736129 ## [7] -0.8761517 0.2199888 -1.9485221 -0.2691319 from the normal distribution with designated mean and standard deviation: rnorm(10,mean=42,sd=12) ## [1] 19.41253 51.93365 30.76463 32.11977 27.47805 25.65418 42.46047 ## [8] 41.97363 39.34858 35.18764 A poisson distribution with mean 2: rpois(10,lambda = 2) ## [1] 0 2 4 1 2 3 4 1 3 2 and the \\(\\chi^2\\) distribution with 1 degree of freedom: rchisq(10, df=1) ## [1] 3.910081627 0.064539643 0.811388855 0.008339997 0.001925748 ## [6] 0.019469490 4.896953308 2.532720573 0.138542269 0.145134578 There are several more distributions that if you need to access random numbers, quantiles, probability densities, and cumulative density values are available. 2.1.1 Coercion to Numeric All data types have the potential ability to take another variable and coerce it into their type. Some combinations make sense, and some do not. For example, if you load in a CSV data file using read_csv(), and at some point a stray non-numeric character was inserted into one of the cells on your spreadsheet, R will interpret the entire column as a character type rather than as a numeric type. This can be a very frustrating thing, spreadsheets should generally be considered evil as they do all kinds of stuff behind the scenes and make your life less awesome. Here is an example of coercion of some data that is initially defined as a set of characters x &lt;- c(&quot;42&quot;,&quot;99&quot;) x ## [1] &quot;42&quot; &quot;99&quot; and is coerced into a numeric type using the as.numeric() function. y &lt;- as.numeric( x ) y ## [1] 42 99 It is a built-in feature of the data types in R that they all have (or should have if someone is producing a new data type and is being courteous to their users) an as.X() function. This is where the data type decides if the values asked to be coerced are reasonable or if you need to be reminded that what you are asking is not possible. Here is an example where I try to coerce a non-numeric variable into a number. x &lt;- &quot;The night is dark and full of terrors...&quot; as.numeric( x ) ## [1] NA By default, the result should be NA (missing data/non-applicable) if you ask for things that are not possible. 2.2 Characters A collection of letters, number, and or punctuation is represented as a character data type. These are enclosed in either single or double quotes and are considered a single entity. For example, my name can be represented as: prof &lt;- &quot;Rodney J. Dyer&quot; prof ## [1] &quot;Rodney J. Dyer&quot; In R, character variables are considered to be a single entity, that is the entire prof variable is a single unit, not a collection of characters. This is in part due to the way in which vectors of variables are constructed in the language. For example, if you are looking at the length of the variable I assigned my name to you see length(prof) ## [1] 1 which shows that there is only one ‘character’ variable. If, as is often the case, you are interested in knowing how many characters are in the variable prof, then you use the nchar(prof) ## [1] 14 function instead. This returns the number of characters (even the non-printing ones like tabs and spaces. nchar(&quot; \\t &quot;) ## [1] 3 As all other data types, you can define a vector of character values using the c() function. x &lt;- &quot;I am&quot; y &lt;- &quot;not&quot; z &lt;- &#39;a looser&#39; terms &lt;- c(x,y,z) terms ## [1] &quot;I am&quot; &quot;not&quot; &quot;a looser&quot; And looking at the length() and nchar() of this you can see how these operations differ. length(terms) ## [1] 3 nchar(terms) ## [1] 4 3 8 2.2.1 Concatenation of Characters Another common use of characters is concatenating them into single sequences. Here we use the function paste() and can set the separators (or characters that are inserted between entities when we collapse vectors). Here is an example, entirely fictional and only provided for instructional purposes only. paste(terms, collapse=&quot; &quot;) ## [1] &quot;I am not a looser&quot; paste(x,z) ## [1] &quot;I am a looser&quot; paste(x,z,sep=&quot; not &quot;) ## [1] &quot;I am not a looser&quot; 2.2.2 Coercion to Characters A character data type is often the most basal type of data you can work with. For example, consider the case where you have named sample locations. These can be kept as a character data type or as a factor (see below). There are benefits and drawbacks to each representation of the same data (see below). By default (as of the version of R I am currently using when writing this book), if you use a function like read_table() to load in an external file, columns of character data will be treated as factors. This can be good behavior if all you are doing is loading in data and running an analysis, or it can be a total pain in the backside if you are doing more manipulative analyses. Here is an example of coercing a numeric type into a character type using the as.character() function. x &lt;- 42 x ## [1] 42 y &lt;- as.character(x) y ## [1] &quot;42&quot; 2.3 Factors A factor is a categorical data type. If you are coming from SAS, these are class variables. If you are not, then perhaps you can think of them as mutually exclusive classifications. For example, an sample may be assigned to one particular locale, one particular region, and one particular species. Across all the data you may have several species, regions, and locales. These are finite, and defined, sets of categories. One of the more common headaches encountered by people new to R is working with factor types and trying to add categories that are not already defined. Since factors are categorical, it is in your best interest to make sure you label them in as descriptive as a fashion as possible. You are not saving space or cutting down on computational time to take shortcuts and label the locale for Rancho Santa Maria as RSN or pop3d or 5. Our computers are fast and large enough, and our programmers are cleaver enough, to not have to rename our populations in numeric format to make them work (hello STRUCTURE I’m calling you out here). The only thing you have to loose by adopting a reasonable naming scheme is confusion in your output. To define a factor type, you use the function factor() and pass it a vector of values. region &lt;- c(&quot;North&quot;,&quot;North&quot;,&quot;South&quot;,&quot;East&quot;,&quot;East&quot;,&quot;South&quot;,&quot;West&quot;,&quot;West&quot;,&quot;West&quot;) region &lt;- factor( region ) region ## [1] North North South East East South West West West ## Levels: East North South West When you print out the values, it shows you all the levels present for the factor. If you have levels that are not present in your data set, when you define it, you can tell R to consider additional levels of this factor by passing the optional levels= argument as: region &lt;- factor( region, levels=c(&quot;North&quot;,&quot;South&quot;,&quot;East&quot;,&quot;West&quot;,&quot;Central&quot;)) region ## [1] North North South East East South West West West ## Levels: North South East West Central If you try to add a data point to a factor list that does not have the factor that you are adding, it will give you an error (or ‘barf’ as I like to say). region[1] &lt;- &quot;Bob&quot; Now, I have to admit that the Error message in its entirety, with its “[&lt;-.factor(*tmp*, 1, value = “Bob”)“` part is, perhaps, not the most informative. Agreed. However, the “invalid factor level” does tell you something useful. Unfortunately, the programmers that put in the error handling system in R did not quite adhere to the spirit of the “fail loudly” mantra. It is something you will have to get good at. Google is your friend, and if you post a questions to (http://stackoverflow.org) or the R user list without doing serious homework, put on your asbestos shorts! Unfortunately, the error above changed the first element of the region vector to NA (missing data). I’ll turn it back before we move too much further. region[1] &lt;- &quot;North&quot; Factors in R can be either unordered (as say locale may be since locale A is not &gt;, =, or &lt; locale B) or they may be ordered categories as in Small &lt; Medium &lt; Large &lt; X-Large. When you create the factor, you need to indicate if it is an ordered type (by default it is not). If the factors are ordered in some way, you can also create an ordination on the data. If you do not pass a levels= option to the factors() function, it will take the order in which they occur in data you pass to it. If you want to specify an order for the factors specifically, pass the optional levels= and they will be ordinated in the order given there. region &lt;- factor( region, ordered=TRUE, levels = c(&quot;West&quot;, &quot;North&quot;, &quot;South&quot;, &quot;East&quot;) ) region ## [1] North North South East East South West West West ## Levels: West &lt; North &lt; South &lt; East 2.3.1 Missing Levels in Factors There are times when you have a subset of data that do not have all the potential categories. subregion &lt;- region[ 3:9 ] subregion ## [1] South East East South West West West ## Levels: West &lt; North &lt; South &lt; East table( subregion ) ## subregion ## West North South East ## 3 0 2 2 2.4 Logical Types A logical type is either TRUE or FALSE, there is no in-between. It is common to use these types in making decisions (see if-else decisions) to check a specific condition being satisfied. To define logical variables you can either use the TRUE or FALSE directly canThrow &lt;- c(FALSE, TRUE, FALSE, FALSE, FALSE) canThrow ## [1] FALSE TRUE FALSE FALSE FALSE or can implement some logical condition stable &lt;- c( &quot;RGIII&quot; == 0, nchar(&quot;Marshawn&quot;) == 8) stable ## [1] FALSE TRUE on the variables. Notice here how each of the items is actually evaluated as to determine the truth of each expression. In the first case, the character is not equal to zero and in the second, the number of characters (what nchar() does) is indeed equal to 8 for the character string “Marshawn”. It is common to use logical types to serve as indices for vectors. Say for example, you have a vector of data that you want to select some subset from. data &lt;- rnorm(20) data ## [1] -0.060330148 1.757068149 0.019086111 -0.005796285 0.496136437 ## [6] 0.428455315 -0.949846630 -0.667583249 -0.876554929 -0.530485211 ## [11] 0.369259638 0.019481911 0.088226466 1.474884106 0.722016145 ## [16] -1.917560964 1.357659975 0.470550413 0.037420498 0.704237538 Perhaps you are on interested in the non-negative values data[ data &gt; 0 ] ## [1] 1.75706815 0.01908611 0.49613644 0.42845531 0.36925964 0.01948191 ## [7] 0.08822647 1.47488411 0.72201615 1.35765997 0.47055041 0.03742050 ## [13] 0.70423754 If you look at the condition being passed to as the index data &gt; 0 ## [1] FALSE TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE ## [12] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE you see that individually, each value in the data vector is being evaluated as a logical value, satisfying the condition that it is strictly greater than zero. When you pass that as indices to a vector it only shows the indices that are TRUE. You can coerce a value into a logical if you understand the rules. Numeric types that equal 0 (zero) are FALSE, always. Any non-zero value is considered TRUE. Here I use the modulus operator, %%, which provides the remainder of a division. 1:20 %% 2 ## [1] 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 which used as indices give us data[ (1:20 %% 2) &gt; 0 ] ## [1] -0.06033015 0.01908611 0.49613644 -0.94984663 -0.87655493 ## [6] 0.36925964 0.08822647 0.72201615 1.35765997 0.03742050 You can get as complicated in the creation of indices as you like, even using logical operators such as OR and AND. I leave that as an example for you to play with. "],
["data-containers.html", "3 Data Containers 3.1 Vectors 3.2 Matrices 3.3 Lists 3.4 Data Frames", " 3 Data Containers We almost never work with a single datum1, rather we keep lots of data. Moreover, the kinds of data are often heterogeneous, including categorical (Populations, Regions), continuous (coordinates, rainfall, elevation), imagry (hyperspectral, LiDAR), and perhaps even genetic. R has a very rich set of containers into which we can stuff our data as we work with it. Here these container types are examined and the restrictions and benefits associated with each type are explained. 3.1 Vectors We have already seen several examples of several vectors in action (see the introduction to Numeric data types for example). A vector of objects is simply a collection of them, often created using the c() function (c for combine). Vectorized data is restricted to having homogeneous data types—you cannot mix character and numeric types in the same vector. If you try to mix types, R will either coerce your data into a reasonable type x &lt;- c(1,2,3) x ## [1] 1 2 3 y &lt;- c(TRUE,TRUE,FALSE) y ## [1] TRUE TRUE FALSE z &lt;- c(&quot;I&quot;,&quot;am&quot;,&quot;not&quot;,&quot;a&quot;,&quot;looser&quot;) z ## [1] &quot;I&quot; &quot;am&quot; &quot;not&quot; &quot;a&quot; &quot;looser&quot; or coearce them into one type that is amenable to all the types of data that you have given it. In this example, a Logical, Character, Constant, and Function are combined resulting in a vector output of type Character. w &lt;- c(TRUE, &quot;1&quot;, pi, ls()) w ## [1] &quot;TRUE&quot; &quot;1&quot; &quot;3.14159265358979&quot; ## [4] &quot;canThrow&quot; &quot;data&quot; &quot;f&quot; ## [7] &quot;file&quot; &quot;files&quot; &quot;i&quot; ## [10] &quot;idx&quot; &quot;inst_pkgs&quot; &quot;item&quot; ## [13] &quot;libraries&quot; &quot;library&quot; &quot;name&quot; ## [16] &quot;pkgs&quot; &quot;pkgs_df&quot; &quot;prof&quot; ## [19] &quot;region&quot; &quot;s&quot; &quot;stable&quot; ## [22] &quot;subregion&quot; &quot;terms&quot; &quot;title&quot; ## [25] &quot;to_install&quot; &quot;value&quot; &quot;x&quot; ## [28] &quot;y&quot; &quot;z&quot; class(w) ## [1] &quot;character&quot; Accessing elements within a vector are done using the square bracket [] notation. All indices (for vectors and matrices) start at 1 (not zero as is the case for some languages). Getting and setting the components within a vector are accomplished using numeric indices with the assignment operators just like we do for variables containing a single value. x ## [1] 1 2 3 x[1] &lt;- 2 x[3] &lt;- 1 x ## [1] 2 2 1 x[2] ## [1] 2 A common type of vector is that of a sequences. We use sequences all the time, to iterate through a list, to counting generations, etc. There are a few ways to generate sequences, depending upon the step sequence. For a sequence of whole numbers, the easiest is through the use of the colon operator. x &lt;- 1:6 x ## [1] 1 2 3 4 5 6 This provides a nice shorthand for getting the values X:Y from X to Y, inclusive. It is also possible to go backwards using this operator, counting down from X to Y as in: x &lt;- 5:2 x ## [1] 5 4 3 2 The only constraint here is that we are limited to a step size of 1.0. It is possible to use non-integers as the bounds, it will just count up by 1.0 each time. x &lt;- 3.2:8.4 x ## [1] 3.2 4.2 5.2 6.2 7.2 8.2 If you are interested in making a sequence with a step other than 1.0, you can use the seq() function. If you do not provide a step value, it defaults to 1.0. y &lt;- seq(1,6) y ## [1] 1 2 3 4 5 6 But if you do, it will use that instead. z &lt;- seq(1,20,by=2) z ## [1] 1 3 5 7 9 11 13 15 17 19 It is also possible to create a vector of objects as repetitions using the rep() (for repeat) function. rep(&quot;Beetlejuice&quot;,3) ## [1] &quot;Beetlejuice&quot; &quot;Beetlejuice&quot; &quot;Beetlejuice&quot; If you pass a vector of items to rep(), it can repeat these as either a vector being repeated (the default value) x &lt;- c(&quot;No&quot;,&quot;Free&quot;,&quot;Lunch&quot;) rep(x,time=3) ## [1] &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;No&quot; &quot;Free&quot; &quot;Lunch&quot; or as each item in the vector repeated. rep(x,each=3) ## [1] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Free&quot; &quot;Free&quot; &quot;Free&quot; &quot;Lunch&quot; &quot;Lunch&quot; &quot;Lunch&quot; 3.2 Matrices A matrix is a 2- or higher dimensional container, most commonly used to store numeric data types. There are some libraries that use matrices in more than two dimensions (rows and columns and sheets), though you will not run across them too often. Here I restrict myself to only 2-dimensional matrices. You can define a matrix by giving it a set of values and an indication of the number of rows and columns you want. The easiest matrix to try is one with empty values: matrix(nrow=2, ncol=2) ## [,1] [,2] ## [1,] NA NA ## [2,] NA NA Perhaps more useful is one that is pre-populated with values. matrix(1:4, nrow=2 ) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Notice that here, there were four entries and I only specified the number of rows required. By default the ‘filling-in’ of the matrix will proceed down column (by-column). In this example, we have the first column with the first two entries and the last two entries down the second column. If you want it to fill by row, you can pass the optional argument matrix(1:4, nrow=2, byrow=TRUE) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 and it will fill by-row. When filling matrices, the default size and the size of the data being added to the matrix are critical. For example, I can create a matrix as: Y &lt;- matrix(c(1,2,3,4,5,6),ncol=2,byrow=TRUE) Y ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 or X &lt;- matrix(c(1,2,3,4,5,6),nrow=2) X ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 and both produce a similar matrix, only transposed. X == t(Y) ## [,1] [,2] [,3] ## [1,] TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE In the example above, the number of rows (or columns) was a clean multiple of the number of entries. However, if it is not, R will fill in values. X &lt;- matrix(c(1,2,3,4,5,6),ncol=4, byrow=TRUE) Notice how you get a warning from the interpreter. But that does not stop it from filling in the remaining slots by starting over in the sequence of numbers you passed to it. X ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 1 2 The dimensionality of a matrix (and data.frame as we will see shortly) is returned by the dim() function. This will provide the number of rows and columns as a vector. dim(X) ## [1] 2 4 Accessing elements to retrieve or set their values within a matrix is done using the square brackets just like for a vector but you need to give [row,col] indices. Again, these are 1-based so that X[1,3] ## [1] 3 is the entry in the 1st row and 3rd column. You can also use ‘slices’ through a matrix to get the rows X[1,] ## [1] 1 2 3 4 or columns X[,3] ## [1] 3 1 of data. Here you just omit the index for the entity you want to span. Notice that when you grab a slice, even if it is a column, is given as a vector. length(X[,3]) ## [1] 2 You can grab a sub-matrix using slices if you give a range (or sequence) of indices. X[,2:3] ## [,1] [,2] ## [1,] 2 3 ## [2,] 6 1 If you ask for values from a matrix that exceed its dimensions, R will give you an error. X[1,8] ## Error in X[1, 8] : subscript out of bounds ## Calls: &lt;Anonymous&gt; ... handle -&gt; withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval ## Execution halted There are a few cool extensions of the rep() function that can be used to create matrices as well. They are optional values that can be passed to the function. times=x: This is the default option that was occupied by the ‘3’ in the example above and represents the number of times that first argument will be repeated. each=x This will take each element in the first argument are repeat them each times. length.out=x: This make the result equal in length to x. In combination, these can be quite helpful. Here is an example using numeric sequences in which it is necessary to find the index of all entries in a 3x2 matrix. To make the indices, I bind two columns together using cbind(). There is a matching row binding function, denoted as rbind() (perhaps not so surprisingly). What is returned is a matrix indices &lt;- cbind( rep(1:2, each=3), rep(1:3,times=2), rep(5,length.out=6) ) indices ## [,1] [,2] [,3] ## [1,] 1 1 5 ## [2,] 1 2 5 ## [3,] 1 3 5 ## [4,] 2 1 5 ## [5,] 2 2 5 ## [6,] 2 3 5 3.3 Lists A list is a type of vector but is indexed by ‘keys’ rather than by numeric indices. Moreover, lists can contain heterogeneous types of data (e.g., values of different class), which is not possible in a vector type. For example, consider the list theList &lt;- list( x=seq(2,40, by=2), dog=LETTERS[1:5], hasStyle=logical(5) ) summary(theList) ## Length Class Mode ## x 20 -none- numeric ## dog 5 -none- character ## hasStyle 5 -none- logical which is defined with a numeric, a character, and a logical component. Each of these entries can be different in length as well as type. Once defined, the entries may be observed as: theList ## $x ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 ## ## $dog ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; ## ## $hasStyle ## [1] FALSE FALSE FALSE FALSE FALSE Once created, you can add variables to the list using the $-operator followed by the name of the key for the new entry. theList$my_favoriate_number &lt;- 2.9 + 3i or use double brackets and the name of the variable as a character string. theList[[&quot;lotto numbers&quot;]] &lt;- rpois(7,lambda=42) The keys currently in the list are given by the names() function names(theList) ## [1] &quot;x&quot; &quot;dog&quot; &quot;hasStyle&quot; ## [4] &quot;my_favoriate_number&quot; &quot;lotto numbers&quot; Getting and setting values within a list are done the same way using either the $-operator theList$x ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 theList$x[2] &lt;- 42 theList$x ## [1] 2 42 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 or the double brackets theList[[&quot;x&quot;]] ## [1] 2 42 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 or using a numeric index, but that numeric index is looks to the results of names() to figure out which key to use. theList[[2]] ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; The use of the double brackets in essence provides a direct link to the variable in the list whose name is second in the names() function (dog in this case). If you want to access elements within that variable, then you add a second set of brackets on after the double ones. theList[[1]][3] ## [1] 6 This deviates from the matrix approach as well as from how we access entries in a data.frame (described next). It is not a single square bracket with two indices, that gives you an error: theList[1,3] ## Error in theList[1, 3] : incorrect number of dimensions ## Calls: &lt;Anonymous&gt; ... handle -&gt; withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval ## Execution halted List are rather robust objects that allow you to store a wide variety of data types (including nested lists). Once you get the indexing scheme down, it they will provide nice solutions for many of your computational needs. 3.4 Data Frames The data.frame is the default data container in R. It is analogous to both a spreadsheet, at least in the way that I have used spreadsheets in the past, as well as a database. If you consider a single spreadsheet containing measurements and observations from your research, you may have many columns of data, each of which may be a different kind of data. There may be factors representing designations such as species, regions, populations, sex, flower color, etc. Other columns may contain numeric data types for items such as latitude, longitude, dbh, and nectar sugar content. You may also have specialized columns such as dates collected, genetic loci, and any other information you may be collecting. On a spreadsheet, each column has a unified data type, either quantified with a value or as a missing value, NA, in each row. Rows typically represent the sampling unit, perhaps individual or site, along which all of these various items have been measured or determined. A data.frame is similar to this, at least conceptually. You define a data.frame by designating the columns of data to be used. You do not need to define all of them, more can be added later. The values passed can be sequences, collections of values, or computed parameters. For example: df &lt;- data.frame( ID=1:5, Names=c(&quot;Bob&quot;,&quot;Alice&quot;,&quot;Vicki&quot;,&quot;John&quot;,&quot;Sarah&quot;), Score=100 - rpois(5,lambda=10)) df ## ID Names Score ## 1 1 Bob 90 ## 2 2 Alice 93 ## 3 3 Vicki 87 ## 4 4 John 94 ## 5 5 Sarah 87 You can see that each column is a unified type of data and each row is equivalent to a record. Additional data columns may be added to an existing data.frame as: df$Passed_Class &lt;- c(TRUE,TRUE,TRUE,FALSE,TRUE) Since we may have many (thousands?) of rows of observations, a summary() of the data.frame can provide a more compact description. summary(df) ## ID Names Score Passed_Class ## Min. :1 Alice:1 Min. :87.0 Mode :logical ## 1st Qu.:2 Bob :1 1st Qu.:87.0 FALSE:1 ## Median :3 John :1 Median :90.0 TRUE :4 ## Mean :3 Sarah:1 Mean :90.2 NA&#39;s :0 ## 3rd Qu.:4 Vicki:1 3rd Qu.:93.0 ## Max. :5 Max. :94.0 We can add columns of data to the data.frame after the fact using the $-operator to indicate the column name. Depending upon the data type, the summary will provide an overview of what is there. 3.4.1 Indexing Data Frames You can access individual items within a data.frame by numeric index such as: df[1,3] ## [1] 90 You can slide indices along rows (which return a new data.frame for you) df[1,] ## ID Names Score Passed_Class ## 1 1 Bob 90 TRUE or along columns (which give you a vector of data) df[,3] ## [1] 90 93 87 94 87 or use the $-operator as you did for the list data type to get direct access to a either all the data or a specific subset therein. df$Names[3] ## [1] Vicki ## Levels: Alice Bob John Sarah Vicki Indices are ordered just like for matrices, rows first then columns. You can also pass a set of indices such as: df[1:3,] ## ID Names Score Passed_Class ## 1 1 Bob 90 TRUE ## 2 2 Alice 93 TRUE ## 3 3 Vicki 87 TRUE It is also possible to use logical operators as indices. Here I select only those names in the data.frame whose score was &gt;90 and they passed popgen. df$Names[df$Score &gt; 90 &amp; df$Passed_Class==TRUE] ## [1] Alice ## Levels: Alice Bob John Sarah Vicki This is why data.frame objects are very database like. They can contain lots of data and you can extract from them subsets that you need to work on. This is a VERY important feature, one that is vital for reproducible research. Keep you data in one and only one place. The word data is plural, datum is singular↩ "],
["manipulating-data.html", "4 Manipulating Data 4.1 Data Import &amp; Export 4.2 Diving Into Data 4.3 Factor &amp; Character Data 4.4 Indexing 4.5 Sorting &amp; Ordering 4.6 Manipulating Data 4.7 Tabulating", " 4 Manipulating Data The sine qua non of of this course is instruction in how how to manipulate data. In this class activity, we will focus on some built-in data sets to allow you to work in R and get more keyboard time in RStudio. There will be a series of questions at the end of the lecture that will ask you to retrieve certain information from various data sets. 4.1 Data Import &amp; Export Raw data is imported into R using read.* functions. There are a wide variety of file formats available, most of which have a corresponding function (e.g., read.csv(), read.delim(), read.dcf(), etc.). For our purposes, we will focus on using comma separated files (*.CSV) as they are the most readily available and can be read by almost all editors and spreadsheet functions. On the lecture webpage, there is a file named iris.csv. Download this file and put it in the same directory as your RStudio session. If you do not know where this is, you can find it by asking R to get its current working directory as: getwd() ## [1] &quot;/Users/rodney/Documents/R/AppliedEnvironmentalStatistics&quot; The same information is also printed across the top of the “Files” pane in the RStudio interface (though it starts from your ‘home’ directory instead of the top of the file path). One way to easily open this location is to select the “Show Folder in New Window” menu item in the “More” menu on that same pane. It will open the folder you are looking at in the file system as a new window for you, then you can drag and drop things into it. Keep in mind that R is running in a specific location on your computer. This working directory is where it looks for stuff if you do not give a complete file path (e.g., ‘C:\\Users...’ or ‘/Users/…’ on winblows and mac, respectively). To load in a CSV file, we can use the function data &lt;- read.csv(&quot;file.csv&quot;) where at a bare minimum, we need to have the name of the file (in the example above it was ‘file.csv’). There are a lot of additional arguments you can pass to read.csv() including: header = TRUE: Does the file have a header row that gives the variable names? sep = &quot;,&quot;: What is the column separator. By default for CSV, it is a comma. quote = &quot;\\&quot;&quot;: Is there text that is quoted within the body of the document? dec = &quot;.&quot;: What is the decimal character? fill = TRUE: Do you want to fill in the empty data cells or do all rows of data have the same amount of data. comment.char = &quot;&quot;: Are there comments in the text? Additional options are available if you look at the help file as: ?read.csv Once you have read the file in (it will complain with an error message if it does not work), then you will have a data.frame object named data (from the example above, you should of course name it something more descriptive). \\(\\;\\) Saving materials in R is a bit easier. If you are needing to export the file back into a CSV format then you can use write.csv() (see ?write.csv for specifics and examples) and it will write the file as a text file. However, if you are only working in R with that file, you can save it as an R object without translating it back and forth through a CSV file. Using the example data from above, you could save the data.frame as an R data object using: save( data, file=&quot;mydata.rda&quot;) and it will save the object. Next time you need it, you can load it in using: load(&quot;mydata.rda&quot;) 4.2 Diving Into Data In this next section, we will walk through the example iris data set and highlight some of the ways that you can manipulate data.frame objects. If you have loaded in the data set as outlined above, we should be able to get a first look at it using the summary() function. summary(flowers) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Each of the columns in the data.frame is named, the first four of which are numerical values and the last one is a species designation. 4.3 Factor &amp; Character Data Depending upon how you have set up your R session, the last column may be either a character type or a factor. By default, some builds of R turn all string characters into factors (a behavior I do not like because there are a lot of data types that I load in that are best defined as non-factors). In this data set though, it is probably best if the Species column were really a factor and not just a character data type. We will play with this data set a bit and we will use the Species as a defining category (i.e., a factor). flowers$Species &lt;- factor( flowers$Species ) summary(flowers) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Notice in the output above how the factor column is tabulated by a count of the number of observations whereas the summary of the character data is just lumped all together. Also remember, if you do not specify the factors as ordered=TRUE then it will assume that they are categorical data without ordinal information. In this case, there is no reason to assume that one of these species types is greater than or less than another one so these are unordered factors. Conversely, if you have imported some data and it was automatically interpreted as factors, you can change a factor back to a character type by using the as.character() function. I find in my interactions with data, I use a lot more textual data than factors so I set the default up to not automatically translate it into factors, only doing so when I need to. In the example above, I replaced the original column of data with the factor version, though you may not need to do that. You can easily add another column of data to the data.frame giving it a new name. In the example below, I take the flowers$Species column and make a new one named Taxonomy by pasting the genus of the flowers onto it. flowers$Taxonomy &lt;- paste( &quot;Iris&quot;, flowers$Species ) summary(flowers) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species Taxonomy ## setosa :50 Length:150 ## versicolor:50 Class :character ## virginica :50 Mode :character ## ## ## \\(\\;\\) 4.4 Indexing The normal workflow in R consists of loading data into your session, manipulating the data, and performing operations—statistical, summary, or graphical–on it (or some subset of it). Each element in the data.frame is indexed by the row and column number. The order of the columns is as shown or can be viewed using names(flowers) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [5] &quot;Species&quot; &quot;Taxonomy&quot; So if we wanted to access the Sepal.Width of 3\\(^{rd}\\) observation, we could use the numerical indices (where the “Sepal.Width” is the second name in this list): flowers[3,2] ## [1] 3.2 or by accessing the 3\\(^{rd}\\) element of the “Sepal.Width” vector. flowers$Sepal.Width[3] ## [1] 3.2 If you pull observations from a data.frame, you either get a new data.frame (if you include more than one column) df &lt;- flowers[,c(2,3,5)] class(df) ## [1] &quot;data.frame&quot; summary(df) ## Sepal.Width Petal.Length Species ## Min. :2.000 Min. :1.000 setosa :50 ## 1st Qu.:2.800 1st Qu.:1.600 versicolor:50 ## Median :3.000 Median :4.350 virginica :50 ## Mean :3.057 Mean :3.758 ## 3rd Qu.:3.300 3rd Qu.:5.100 ## Max. :4.400 Max. :6.900 or a vector of data (if you only have one column or any subset of one column). sepal_width &lt;- flowers$Sepal.Width length(sepal_width) ## [1] 150 class(sepal_width) ## [1] &quot;numeric&quot; sepal_width ## [1] 3.5 3.0 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 3.7 3.4 3.0 3.0 4.0 4.4 3.9 ## [18] 3.5 3.8 3.8 3.4 3.7 3.6 3.3 3.4 3.0 3.4 3.5 3.4 3.2 3.1 3.4 4.1 4.2 ## [35] 3.1 3.2 3.5 3.6 3.0 3.4 3.5 2.3 3.2 3.5 3.8 3.0 3.8 3.2 3.7 3.3 3.2 ## [52] 3.2 3.1 2.3 2.8 2.8 3.3 2.4 2.9 2.7 2.0 3.0 2.2 2.9 2.9 3.1 3.0 2.7 ## [69] 2.2 2.5 3.2 2.8 2.5 2.8 2.9 3.0 2.8 3.0 2.9 2.6 2.4 2.4 2.7 2.7 3.0 ## [86] 3.4 3.1 2.3 3.0 2.5 2.6 3.0 2.6 2.3 2.7 3.0 2.9 2.9 2.5 2.8 3.3 2.7 ## [103] 3.0 2.9 3.0 3.0 2.5 2.9 2.5 3.6 3.2 2.7 3.0 2.5 2.8 3.2 3.0 3.8 2.6 ## [120] 2.2 3.2 2.8 2.8 2.7 3.3 3.2 2.8 3.0 2.8 3.0 2.8 3.8 2.8 2.8 2.6 3.0 ## [137] 3.4 3.1 3.0 3.1 3.1 3.1 2.7 3.2 3.3 3.0 2.5 3.0 3.4 3.0 In addition to numerical values, you can also use logical statements to select subsets of your data. Here is an example of all the data whose flowers$Sepal.Width is less than 3.0cm and those that have sepals as large or bigger than 3.0cm. small_sepals &lt;- flowers[ flowers$Sepal.Width &lt; 3.0, ] big_sepals &lt;- flowers[ flowers$Sepal.Width &gt;= 3.0, ] summary( small_sepals ) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.400 Min. :2.00 Min. :1.300 Min. :0.200 ## 1st Qu.:5.600 1st Qu.:2.50 1st Qu.:4.000 1st Qu.:1.200 ## Median :6.000 Median :2.70 Median :4.500 Median :1.400 ## Mean :5.953 Mean :2.64 Mean :4.509 Mean :1.449 ## 3rd Qu.:6.300 3rd Qu.:2.80 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.700 Max. :2.90 Max. :6.900 Max. :2.400 ## Species Taxonomy ## setosa : 2 Length:57 ## versicolor:34 Class :character ## virginica :21 Mode :character ## ## ## summary( big_sepals ) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :3.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.000 1st Qu.:3.000 1st Qu.:1.500 1st Qu.:0.200 ## Median :5.600 Median :3.200 Median :1.900 Median :0.500 ## Mean :5.776 Mean :3.313 Mean :3.298 Mean :1.046 ## 3rd Qu.:6.500 3rd Qu.:3.500 3rd Qu.:5.200 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.700 Max. :2.500 ## Species Taxonomy ## setosa :48 Length:93 ## versicolor:16 Class :character ## virginica :29 Mode :character ## ## ## \\(\\;\\) There are two ways you can also merge data.frame objects. You can add data onto the bottom of the data.frame by using the rbind() function (row-binding). This requires that both data.frame objects have the same column names (and in the same order). all_sepals &lt;- rbind( small_sepals, big_sepals ) summary( all_sepals ) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species Taxonomy ## setosa :50 Length:150 ## versicolor:50 Class :character ## virginica :50 Mode :character ## ## ## You can also merge together two data.frames that have a common column index. This may be necessary for cases where you have different kinds of observations that need to be merged into a single data.frame. Here is an example where I have some data on my sampling sites sites &lt;- data.frame( Site=c(&quot;Olympia&quot;,&quot;Richmond&quot;), Latitude=c(47.0379,37.5407), Longitude=c(122.9007,77.4360) ) sites ## Site Latitude Longitude ## 1 Olympia 47.0379 122.9007 ## 2 Richmond 37.5407 77.4360 and another set of data that has some observations on samples taken from each site. data &lt;- data.frame( Site=c(&quot;Olympia&quot;,&quot;Olympia&quot;,&quot;Olympia&quot;,&quot;Richmond&quot;,&quot;Richmond&quot;)) data$Measurement &lt;- c(12,22,35,56,46) data ## Site Measurement ## 1 Olympia 12 ## 2 Olympia 22 ## 3 Olympia 35 ## 4 Richmond 56 ## 5 Richmond 46 If I wanted to merge these two data.frame objects into a single one, incorporating each of the columns of data in both, I would df &lt;- merge( sites, data ) df ## Site Latitude Longitude Measurement ## 1 Olympia 47.0379 122.9007 12 ## 2 Olympia 47.0379 122.9007 22 ## 3 Olympia 47.0379 122.9007 35 ## 4 Richmond 37.5407 77.4360 56 ## 5 Richmond 37.5407 77.4360 46 Here the merge() function looks for a column that has the same name in both data.frame objects. In this case it was “Site”. It then uses that as an index to merge both together into a new object. 4.5 Sorting &amp; Ordering The order in which rows of observations are in the data.frame is determined by their placement in the original file. If you look at the data, it seems to be sorted by flowers$Species but nothing after that. head(flowers) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Taxonomy ## 1 5.1 3.5 1.4 0.2 setosa Iris setosa ## 2 4.9 3.0 1.4 0.2 setosa Iris setosa ## 3 4.7 3.2 1.3 0.2 setosa Iris setosa ## 4 4.6 3.1 1.5 0.2 setosa Iris setosa ## 5 5.0 3.6 1.4 0.2 setosa Iris setosa ## 6 5.4 3.9 1.7 0.4 setosa Iris setosa You can sort the whole data.frame by asking for a copy of it with a specific order based upon the columns. Here I will re-assign the data.frame but this time ordered by flowers$Sepal.Length. flowers &lt;- flowers[ order(flowers$Sepal.Length), ] head(flowers) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Taxonomy ## 14 4.3 3.0 1.1 0.1 setosa Iris setosa ## 9 4.4 2.9 1.4 0.2 setosa Iris setosa ## 39 4.4 3.0 1.3 0.2 setosa Iris setosa ## 43 4.4 3.2 1.3 0.2 setosa Iris setosa ## 42 4.5 2.3 1.3 0.3 setosa Iris setosa ## 4 4.6 3.1 1.5 0.2 setosa Iris setosa Note the row indices (the column on the far left), they indicate the original order in which the observations were put into the data.frame. The smallest sepal length in the data set was originally the 14\\(^{th}\\) observation. You see from the example above that the addition of new columns to a data.frame result in them being put on the right-hand side data.frame (e.g., flowers$Taxonomy is the last column of the output). If you want to rearrange the columns of the data.frame, you do so by manually re-arranging the indices on those columns. For example, if I wanted to make the numerical data the last four columns instead of the first two, I would specify it as: flowers &lt;- flowers[ , c(6,5,1,2,3,4) ] summary(flowers) ## Taxonomy Species Sepal.Length Sepal.Width ## Length:150 setosa :50 Min. :4.300 Min. :2.000 ## Class :character versicolor:50 1st Qu.:5.100 1st Qu.:2.800 ## Mode :character virginica :50 Median :5.800 Median :3.000 ## Mean :5.843 Mean :3.057 ## 3rd Qu.:6.400 3rd Qu.:3.300 ## Max. :7.900 Max. :4.400 ## Petal.Length Petal.Width ## Min. :1.000 Min. :0.100 ## 1st Qu.:1.600 1st Qu.:0.300 ## Median :4.350 Median :1.300 ## Mean :3.758 Mean :1.199 ## 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :6.900 Max. :2.500 4.6 Manipulating Data As we saw above, you can change data either in-place or by adding new columns (and you could also drop columns by reordering them and just not include all the columns in the column indices). You can also perform operations on columns of data, again either in-place, as an additional column, or not connected at all with the data.frame. To do it in-place, you simply re-assign the values after the calculation. For example, here is how I would subtract the average “Sepal.Length” from all the observations. ave_sepal_length &lt;- mean( flowers$Sepal.Length ) standardized_sepal_lengths &lt;- flowers$Sepal.Length - ave_sepal_length summary( standardized_sepal_lengths ) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.54300 -0.74330 -0.04333 0.00000 0.55670 2.05700 Which should have a mean of zero, right? \\(\\;\\) 4.7 Tabulating We can finish up this exploration of the iris data set by doing some summaries. One of the main strengths of R is that it is a functional language. This may not mean much to you now, but in the long run you will find that you can perform opertions on large data sets by merging together cleaver use of indices and the application of one or more functions. Consider the case where we are trying to find the mean for each of the three species. Proceedurally, we must do the following general steps: Go through each row of the data and figure out which species we are looking at. Add the length of this observation to a tally variable, one for each species. Count how many observations of each species we have in the data.frame. Divide the approporiate tally variable by the number of observations. Return or print out the results. Not a trivial thing to do. If you were going to to this literally, you would have to set up three tally variables, three observation count variables, and then do a loop and run through all the observations making decisions on which observation is which and where to add the tally and count. Fortunately for us, this is not that difficult of an issue in R for the following two reasons: 1. We have a function mean() that estimates the mean of a set of variables. 2. We have a factor varaible in flowers that differentiates between species. As such we can then ask R to estimate the mean in “Sepal.Length” by “Species” using mean_sepal_length &lt;- by( flowers$Sepal.Length, flowers$Species, mean) mean_sepal_length ## flowers$Species: setosa ## [1] 5.006 ## -------------------------------------------------------- ## flowers$Species: versicolor ## [1] 5.936 ## -------------------------------------------------------- ## flowers$Species: virginica ## [1] 6.588 If you read this literally, it says, “Wit Sepal Length, partition the data by Species and pass it to the function mean”. This shorthand is important in that it highlights the flexibility and utility of applying functions to our data. With a few keystrokes, we can accomplish a lot of computational progress! You could find these values by slicing as: # literally grab the mean of all sepal lengths WHERE species is identical to X mean_setosa &lt;- mean( flowers$Sepal.Length[ flowers$Species == &quot;setosa&quot; ] ) mean_versicolor &lt;- mean( flowers$Sepal.Length[ flowers$Species == &quot;versicolor&quot; ] ) mean_virginica &lt;- mean( flowers$Sepal.Length[ flowers$Species == &quot;virginica&quot; ] ) mean_sepal_lengths_long &lt;- c(setosa=mean_setosa, versicolor=mean_versicolor, virginica=mean_virginica ) mean_sepal_lengths_long ## setosa versicolor virginica ## 5.006 5.936 6.588 But it sure does look like a lot more code to write! In general, we should strive to keep our code as easy to understand as possible. In the end, you will be reading your own code and you must be able to easily understand it at some random time in the future. For this to happen, you really need to clear on what you are trying to do. \\(\\;\\) "],
["programming.html", "5 Programming 5.1 Function Writing 5.2 Variable Scope 5.3 Decision Making 5.4 The if-else Pattern 5.5 Flow Control", " 5 Programming One of the strengths of R as an analysis platform is that it is a language rather than a program. With programs, such as SPSS &amp; JMP, you are limited by the functionality that the designers thought would be necessary to meet the broadest audience. In R, you can rely upon simple functions or you can create entire analysis and simulation programs de novo. To do this, we need to dig into flow control and decision making processes, both of which you need for doing more in-depth programming. 5.1 Function Writing Here we look at how to create an R function. Writing small functions like this is a huge benefit to you as an analyst and this is a great place to start. A function in R is defined as: function_name &lt;- function( arguments ) { Stuff you want the function to do } You define a function name for whatever you like (given you use the names) and assign it the stuff to the right. In R, the function named function() is a special one, it tells R that you are about to create a little routine and you want that set of code to be available to you for later use under the name of whatever you named it. This allows a tremendous amount of flexibility as you develop your own set of routines and analyses for your work. The part that actually does stuff is after the function call. It may be that the function that you create does need some data (those are the arguments) or you may not need any input in to the function (in which you pass no arguments). It all depends upon what you are creating. The key to understanding functions is that they are encapsulations of code—a shortcut for a sequence of instructions if you will not have to type over and over again. The less typing you do, the lower the probability that you will have errors (and all code has errors). Here is an example of some code that I’m going to develop into a function. This function will allow me to determine if one genotype could possibly be the offspring of the other genotype. library(gstudio) loc1 &lt;- locus( c(128,130) ) loc2 &lt;- locus( c(128,128) ) cat( loc1, loc2 ) ## 128:130 128:128 We start out with two loci, a 128:130 heterozygote and a 128:128 homozygote. These may represent repeat motifs at a microsatellite locus or some other co-dominant genotype. First, I’ll break the locus into a vector of genotypes. off.alleles &lt;- alleles( loc1 ) off.alleles ## [1] &quot;128&quot; &quot;130&quot; mom.alleles &lt;- alleles( loc2 ) mom.alleles ## [1] &quot;128&quot; &quot;128&quot; To be a valid potential offspring there should be at least one of the alleles in the parent that matches the allele in the offspring. The intersect() function returns the set of values common to both vectors. shared &lt;- intersect( off.alleles, mom.alleles ) shared ## [1] &quot;128&quot; If it has at least one of the alleles present (it could have both if parent and offspring are both the same heterozygote) then you cannot exclude this individual as a potential offspring. If there are no alleles in common, then the value returned is an empty vector. loc3 &lt;- locus( c(132,132)) dad.alleles &lt;- alleles( loc3 ) intersect( mom.alleles, dad.alleles ) ## character(0) This logic can be shoved into a function. You have to wrap it into a set of curly brackets. I use the length of the result from the intersect() to return from the function. Potential values for potential_offspring &lt;- function( parent, offspring ) { off &lt;- alleles( offspring ) par &lt;- alleles( loc2 ) shared &lt;- intersect( off, par ) return( length( shared ) &gt; 0 ) } Now, you can call this function anytime you need, just passing it two genotypes. If they can be offspring it returns TRUE, as in the comparison between 128:130 and 128:128 genotypes. potential_offspring(loc1, loc2) ## [1] TRUE And it returns FALSE for the comparison between 128:128 and 132:132. potential_offspring(loc2, loc3) ## [1] FALSE 5.2 Variable Scope There is a lot more information on writing functions and we will get into that as we progress through the text. However, it is important that I bring this up now. The value assigned to a variable is defined by its scope. Consider the following code x &lt;- 10 and the function defined as do_it &lt;- function( x ) { x &lt;- x + 10 return( x ) } When I call the function, the variable x that is the argument of the function is not the same variable that is in the environment that I assigned a value of 10. The x in the function argument is what we call “local to that function” in that within the curly brackets that follow (and any number of curly brackets nested within those, the value of x is given whatever was passed to the function. 5.3 Decision Making We interact with our data in many ways and introspection of the values we have in the variables we are working with are of prime importance. Decision making in your code is where you evaluate your data and make a choice of outcomes based upon some criteria. Here is some example data that we can use as we explore the basics of if(), if(){} else{}, and if(){} elif(){} else{} coding patterns. 5.3.1 The if Pattern The most basic version of decision making is asking a single question and if the answer is TRUE then do something. The if(){} function does this and has the form if( CRITERIA ) { DO_SOMETHING } You pass a logical statement (or something that can be coerced into a logical type) to the function as the CRITERIA and if it evaluates to TRUE, then the contents of the DO_SOMETHING are executed. If the value of CRITERIA is not TRUE the DO_SOMETHING is skipped entirely—it is not even seen by the interpreter. Here we can test this out using the loci defined above along with the is_heterozygote() function. This function takes one or more locus objects and returns TRUE/FALSE if they are or are not a heterozygote. is_heterozygote( c(loc1, loc2) ) ## [1] TRUE FALSE If we shove that function into the if() parameters we can use its evaluation of the heterozygous state of the locus to do something interesting, say tell us it is a heterozygote—it is admittedly a contrived example, but hey you try to make easy examples, it is not easy. if( is_heterozygote(loc1) ){ print(&quot;It&#39;s a het!&quot;) } ## [1] &quot;It&#39;s a het!&quot; If the is_heterozygote() function returns a value of FALSE, then the contents of the if() function (the stuff within the curly brackets is skipped entirely. if( is_heterozygote(loc2) ){ print(&quot;It&#39;s a het!&quot;) } Notice, there was no indication of any of that code inside the curly brackets. The if-else Pattern If there are more than on thing you want to potentially do when making a decision, you can add an else clause after the if pattern. Here if is_heterozygote() returns FALSE, the contents of the else{} clause will be executed. Here is the heterozygote example if( is_heterozygote(loc1) ) { cat(loc1, &quot;is a heterozygote&quot;) } else { cat(loc1, &quot;is a homozygote&quot;) } ## 128:130 is a heterozygote and the homozygote one if( is_heterozygote(loc2) ) { cat(loc2, &quot;is a heterozygote&quot;) } else { cat(loc2, &quot;is a homozygote&quot;) } ## 128:128 is a homozygote There is a slightly shorter version of this that is available for the lazy programmer and lets be honest, all programmers are lazy and the more you can accomplish with fewer strokes on the keyboard the better (this is how we got emacs and vim). I generally don’t teach the shortcuts up front, but this one is short and readily apparent so it may be more helpful than confusing. The ifelse() function has three parts, the condition, the result if TRUE, and the result if FALSE. ans &lt;- ifelse( is_heterozygote( c(loc1, loc2)) , &quot;heterozygote&quot;, &quot;Not&quot;) ans ## [1] &quot;heterozygote&quot; &quot;Not&quot; So iterating through the x vector, the condition x&gt;0 is evaluated and if TRUE the sqrt() of the value is returned, else the NA is given. It is compact and easy to use so you may run into it often. 5.4 The if-else Pattern It is possible to test many conditions in a single sequence by stringing together else-if conditions. The point that is important here is that the first condition that evaluates to TRUE will be executed and all remaining ones will be skipped, even if they also are logically TRUE. This means that it is important to figure out the proper order of asking your conditions. Here is an example function that determines if none, one, or both of the genotypes passed to it are heterozygotes. By default, I step through every one of the potential options of available on this comparison. 1. The first is a heterozygote and the second one isn’t 2. The first one isn’t and the second one is 3. Both are heterozygotes 4. The last state (both are not) Here is the function. which_is_het &lt;- function( A, B) { if( is_heterozygote(A) &amp; !is_heterozygote(B) ) { print(&quot;First is heterozygote&quot;) } else if( !is_heterozygote(A) &amp; is_heterozygote(B) ){ print(&quot;Second is heterozygote&quot;) } else if( is_heterozygote(A) &amp; is_heterozygote(B) ){ print(&quot;Both are heterozygotes&quot;) } else { print( &quot;Neither are heterozygotes&quot;) } } It is possible that the order of these CRITERIA could be changed, the important thing to remember is that the sequence of if - else if - else if etc. will terminate the very first time one of the CRITERIA is evaluated to be TRUE. 5.5 Flow Control Flow control is the process of iterating across objects and perhaps doing operations on those objects. The R language has several mechanisms that you can use to control the flow of a script or bit of code. 5.5.1 The for() Loop x &lt;- c(3,8,5,4,6) x ## [1] 3 8 5 4 6 You can iterate through this vector using a for() loop. This is a simple function that has the form: for( SOME_SEQUENCE ){ DO_SOMETHING } Where the SOME_SEQUENCE component is a sequence of values either specified OR calculated and the DO_SOMETHING is the thing you want to do with each of the values in the sequence. Usually, there is a variable defined in the SOME_SEQUENCE component and the value of that variable is used. Here are a few examples. The first goes through the existing vector directly and assigns (in sequential order) the entries of ‘x’ to the variable val. We can then do whatever we want with the value in val (though if we change it, nothing happens to the original x vector). for( val in x ){ print(val) } ## [1] 3 ## [1] 8 ## [1] 5 ## [1] 4 ## [1] 6 We can also specify a sequence directly and then use it as an index. Here I use an index variable named i to take on the integer seqeunce equal in length to the length of the original x variable. Then I can iterate through the original vector and use that index variable to grab the value I want. for( i in 1:length(x)){ print( x[i] ) } ## [1] 3 ## [1] 8 ## [1] 5 ## [1] 4 ## [1] 6 Both give us the same output, namely a way to go through the variable x. However, there may be a need to use the latter approach in your calculations. For example, perhaps I want to do some other operation on the values. In this very contrived example that follows, I want to perform operations on the values in x depending on if they are even or odd. For the odd ones, I add the corresponding value in y and if not I subtract it. Sure, this is totally contrived and I cannot think of a reason why I would be doing this, but if I need to know what index (row, column or whatever) an entry is during the iteration process, then I need to use this approach over the for( val in x) approach. y &lt;- 1:5 for( i in 1:length(x)){ if( x[i] %% 2) print( x[i] + y[i]) else print( x[i] - y[i] ) } ## [1] 4 ## [1] 6 ## [1] 8 ## [1] 0 ## [1] 1 5.5.2 Short Circuiting the Loop It is possible to short circuit the looping process using the keywords next and break, though in my programming style, I consider their use in my source files as evidence of inelegant code. That said, you may need them on occasion. The next keyword basically stops all commands after that during the current iteration of the loop. It does not terminate the loop itself, it just stops the commands that follow it this time through. Here is an example that uses the modulus operator, %% (e.g., the remainder after division), to print out only those numbers that are divisible by three. for( i in 1:20 ){ if( i %% 3 ) next cat(&quot;The value of i =&quot;,i,&quot;\\n&quot;) } ## The value of i = 3 ## The value of i = 6 ## The value of i = 9 ## The value of i = 12 ## The value of i = 15 ## The value of i = 18 The use of break to exit the loop entirely is perhaps more commonly encountered. When this keyword is encountered, the loop terminates immediately, as if it reached the send of the sequence. for( i in 1:10){ if( i &gt; 2 ) break cat(&quot;The value of i=&quot;,i,&quot;\\n&quot;) } ## The value of i= 1 ## The value of i= 2 "],
["markdown.html", "6 Markdown 6.1 Markdown 6.2 Marking up Text 6.3 Inserting R Code Chunks 6.4 Variables in Text", " 6 Markdown Writing code and the associated text around the analyses and your data is an odd job that munges together some programming skills, a lot of copy-and-paste, saving images, menu jujitsu (Insert \\(\\to\\) Image \\(\\to\\) From Computer…), etc. One of the things that we should strive to do is to minimize the amount of stoooopid work that we have to do in our daily lives, right? Markdown is a way to create textual output that can be magically turned into many types of output. In this section, we start looking at some markdown examples that integrate our data, analysis code, and output into a single document that makes our lives suck just a little bit less . 6.1 Markdown Consider the case where you are making a report using a word processor like Word or Pages. The most basic part of that document is the actual text that you use in the document. Certain parts of that text represent different ‘landmarks’ in the paper, titles and headers are typically of a different typeface, images are saved from a different program, etc. If you are to include code, results from analyses, images of output and other stuff that isn’t directly typed into the document, you must do that outside the word processor and somehow insert it into that document. This moving between the processor where you are typing the actual content and other external sources of content and mushing them together is pretty much common place. In addition to this work flow, at the end of the day you spend a lot of time working on presentation style. Images and tables moving between page breaks, manually renumbering equations and figures, trying to figure out where captions and figure legends go, and at the end of the day, making sure the statistical work that you did stays in sync with how you are presenting the output in the document. One change in your analysis may percolate to many changes throughout the document that you must manually fix. Perhaps more importantly though, at the end of the day, you are left with a single Word document. What if you want to take those analyses and make a presentation, poster, website, docbook, or pdf out of it? What if you want to make automatic reports on continuous data sources? You have to do this over and over again manually because you are stuck using a word processor that only makes one kind of output. Markdown was designed as a way to create content and display it in many different formats, from the same kind of source. It is a very simple way of creating content that allows you to present it in a wide variety of output formats. Markdown is, at a bare minimum, just text. Anyone can write it using any editor you like. For our purposes, RStudio is a nice editor and the one we will use. A markdown file is saved with the extension .Rmd (or .rmd). To make a markdown document in RStudio, select File \\(\\to\\) New File \\(\\to\\) RMarkdown… and you will be give a dialog of potential types of markdown templates to start from that look like the following image. The main categories on the left are: Document: This is the main category of documents you can prepare. Presentation: You can make html and pdf presentations using markdown. Shiny: Shiny is a way to make interactive graphics and data ‘dashboards’ in R that are hosted on a web server (free ones are available or you can host one on your own server if you have one). Templates: This is where specific themed templates are stored for any of the above type of markdown document. For the time being, lets stick with documents and keep the default setting for HTML. As noted in the dialog, we can switch to the other formats later without a problem and HTML output is perhaps the most versatile for using while we are creating the document. 6.2 Marking up Text In a markdown document, it is all text. As such, we need to do typographic things like make headings, bold, italics, etc. Markdown accomplishes these by using symbols around the textual items you are changing. Here are some examples as an image (if I typed them into this document, which is RMarkdown, they would be converted so you wouldn’t see them). There are a lot more markup options (footnotes, links, citations, etc.) that can be found by selecting Help \\(\\to\\) Markdown Quick Reference in RStudio. 6.3 Inserting R Code Chunks In markdown, code is contained with what is called a ‘chunk’. This chunk can be a block of code (encompassing some kind of data analysis or the creation of some kind of graphical output) or it can be a bit that is inserted inline with your text (e.g., something like P &lt; 0.0323 may be computed and inserted on the fly). You can insert a chunk of code by selecting the R option from the Insert menu just above the editor (see below). There is also a keyboard shortcut that can be used to insert a chunk without going to the menu (and it is much faster), though they are platform dependent. Once you insert a chunk, RStudio provides a highlighted section within which you can insert your code. Everything within the chunk itself is going to be interpreted just as if it were written in a normal R script. 6.3.1 Chunk Options There are several options you can specify that determine the behavior of the chunk. These are inserted within the curly brackets at the top of the chunk. Some of the more common ones include: eval=TRUE/FALSE Determine if the code is actually run. Perhaps you are just demonstrating the code. echo=TRUE/FALSE Determine if the code is shown in the output. We can hide a lot of code ‘behind the scenes’ in our documents and only provide the output as appropriate (e.g., you’d never show the code for the analysis in your manuscript but it can be embedded in the document directly and serve up the results when you compile it). fig.cap=&quot;&quot; Caption for figures and tables. warning=TRUE/FALSE, message=TRUE/FALSE, error=TRUE/FALSE Toggle the visibility of warnings, messages, and errors in the output in the final document. There are many additional options that can be found in the documentation for knitr (the package that actually does the magic of turning the markdown into something else) at http://yihui.name/knitr/options/ 6.4 Variables in Text Variables and values from the R code in your document can be inserted into the text as well. To do this, you enclose the R code you are using in between a backslash plus the letter r (“r&quot;) and a trailing backslash (&quot;”). Here is an image of an example and the output: My all time favorite number is 9 and always has been. "],
["manipulating-data-2.html", "7 Manipulating Data", " 7 Manipulating Data "],
["graphical-display.html", "8 Graphical Display", " 8 Graphical Display "],
["ggplot.html", "9 GGPlot", " 9 GGPlot "],
["spatial-data.html", "10 Spatial Data", " 10 Spatial Data "],
["summary-statistics.html", "11 Summary Statistics 11.1 Central Tendency 11.2 Dispersion 11.3 Ordination 11.4 Hierarchical Clustering", " 11 Summary Statistics In this secion we close out the exploring data section by defining measures of central tendency, dispersion, and explore a few ways to ordinate data. For these data, we will use the iris data set as an example. This data set is built into R and is has the quantitative measurements for Sepal Width, Sepal Length, Petal Length, and Petal Width for 150 samples taken from individuals in three different species of Iris. summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 11.1 Central Tendency Central tendency is a term we use to define the center of gravity of some data. There are several measures of central tendency that we commonly use. In R, only the arithmetic mean is included by default, to estimate the remaining ones, we can install the DescTools package. install.packages(&quot;DescTools&quot;) Once installed, we need to load it into memory. library(DescTools) and we are ready to go. Arithmetic Mean - The arithmetic mean is the ‘average’ that we use on the common vernacular and is defined as: \\[ \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i \\] In R we estimate it using the mean() function. mean( iris$Sepal.Width ) ## [1] 3.057333 We can apply this across groupings in our data using the by() function (as we’ve done previously) to measure the mean of measurements across groups of observations. by( iris$Sepal.Width, iris$Species, mean ) ## iris$Species: setosa ## [1] 3.428 ## -------------------------------------------------------- ## iris$Species: versicolor ## [1] 2.77 ## -------------------------------------------------------- ## iris$Species: virginica ## [1] 2.974 Geometric Mean - The arithmetic mean is not the only way to measure the center of gravity of your data. The geometric mean is defined as the \\(n^{th}\\) root of the product of measurements. \\[ \\mu_{g} = \\left( \\prod_{i=1}^N x_i \\right)^{\\frac{1}{N}} \\] library(DescTools) Gmean(iris$Sepal.Width ) ## [1] 3.026598 The geometric mean is often used to compare values that are multiples or exponential. For example, in the Iris data, it may be more appropriate to evaluate differences in sepal area using the geometric mean rather than the arithmetic one. by( iris$Sepal.Width*iris$Sepal.Length, iris$Species, Gmean ) ## iris$Species: setosa ## [1] 17.01442 ## -------------------------------------------------------- ## iris$Species: versicolor ## [1] 16.27452 ## -------------------------------------------------------- ## iris$Species: virginica ## [1] 19.39149 Harmonic Mean - The harmonic mean is an approach commonly applied to fractional data (percentages) or data that has outliers. It is estimated as the recipricol of the arithmetic mean of reciprocals… \\[ \\mu_{h} = \\frac{1}{\\frac{1}{N} \\left( \\frac{1}{x_1} + \\frac{1}{x_2} + \\cdots + \\frac{1}{x_N} \\right)} \\] The effects of outliers can be seen in the example: x &lt;- c(2,3,5,6,100) mean(x) ## [1] 23.2 Hmean(x) ## [1] 4.132231 In our iris data, we can apply the Hmean as before across species by( iris$Sepal.Width, iris$Species, Hmean ) ## iris$Species: setosa ## [1] 3.385537 ## -------------------------------------------------------- ## iris$Species: versicolor ## [1] 2.733011 ## -------------------------------------------------------- ## iris$Species: virginica ## [1] 2.940087 Median - The median is the center of the data, the value where half of the observations are larger and half are smaller—the 50\\(^{th}\\) quantile. median( iris$Sepal.Width ) ## [1] 3 by( iris$Sepal.Width, iris$Species, median ) ## iris$Species: setosa ## [1] 3.4 ## -------------------------------------------------------- ## iris$Species: versicolor ## [1] 2.8 ## -------------------------------------------------------- ## iris$Species: virginica ## [1] 3 This is a rank-based measure of central tendency and one that we saw in the Normality discussion last week. 11.2 Dispersion In addition to the central tendency, for us to describe the data, we also need to know a bit about the dispersion of values around the center of gravity. Range - The range is the physical separation between the smallest and largest values. range( iris$Sepal.Width ) ## [1] 2.0 4.4 by( iris$Sepal.Width, iris$Species, range ) ## iris$Species: setosa ## [1] 2.3 4.4 ## -------------------------------------------------------- ## iris$Species: versicolor ## [1] 2.0 3.4 ## -------------------------------------------------------- ## iris$Species: virginica ## [1] 2.2 3.8 It is identical to: c( min(iris$Sepal.Width), max( iris$Sepal.Width)) ## [1] 2.0 4.4 Sample Variance - The variance of the data, \\(\\sigma^2\\), is defined as the average distance between the observations and the arithmetic mean of the observations. It is not quite the ‘average’ since we need to punish ourselves for estimation the mean as we’ve lost a degree of freedom. We will come back to this later in a more detailed way, just take it on faith right now. The idealized formula is: \\[ s^2 = \\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\mu)^2 \\] which you should never use when using a computer to estimate the sample variance. There are some significant problems with round-off error that cause this theoretical formula to produce incorrect results. In R we estimate the variance as: var( iris$Sepal.Width ) ## [1] 0.1899794 and can be applied across groups as: by( iris$Sepal.Width, iris$Species, var) ## iris$Species: setosa ## [1] 0.1436898 ## -------------------------------------------------------- ## iris$Species: versicolor ## [1] 0.09846939 ## -------------------------------------------------------- ## iris$Species: virginica ## [1] 0.1040041 where we can see that I. setosa has more variation in sepal width than the other species. Standard Deviation - The units on the variance are not same as the units for the original data, since it is the deviance squared. To interpret the dispersion in a way that is comparable, we take the square root of the variance - a term called the standard deviation. And while it has its own formula sd( iris$Sepal.Width) ## [1] 0.4358663 it is identical to sqrt( var( iris$Sepal.Width)) ## [1] 0.4358663 It is common to use the standard deviation to designate some spread of the data around the mean using error bars. Here is an example using the barplot() and arrows() functions (n.b. I return the value of the x-coordinates from the bar plot function to use in the arrows function). mu &lt;- by( iris$Sepal.Width, iris$Species, mean ) b &lt;- barplot(mu, ylim=c(0,4)) sepal.sd &lt;- by( iris$Sepal.Width, iris$Species, sd ) x &lt;- b y0 &lt;- as.numeric( mu ) y1 &lt;- y0 + as.numeric( sepal.sd) arrows( x,y0,x,y1, angle = 90 ) 11.3 Ordination There is a group of routines that are used to visualize and reformat data called “ordination.” These approaches are not necessarily analyses, they are more commonly used to understand the underlying data or to reduce the dimensionality of multivariate data. In this section, we are going to explore a bit about principal component (PC) rotation. A PC rotation is one that takes the original columns of data and performs a rotation on the values to align onto new ‘synthetic’ axes. Consider the example in the next figure. Here, some bivariate data is plot in 2-space, though this can be done for much higher dimensions of data as well—in fact it is more beneficial with more columns of data and this can be used as a way of reducing the dimensionality of the data while loosing very little (or no) content. The axes of a PC rotation are taken as linear combinations of the existing axes and define a new coordinate set onto which the points are plot. All points are rigidly constrained to keep the same relationship and there is no loss of information. The PC axes are defined by determining the most variable stretch through the data. In the figure, we see the raw data plot onto the X- and Y-axes. The axis of highest variance does not align with either of the original ones, and instead can be defined as a combination of both X- and Y- coordinates. If we take the blue axis as the first PC axis, the coordinate of the points would be taken along that new synthetic axis. The next PC axis is defined as being perpendicular to the previous one(s) and is identified as covering the largest variance in the data as before. This process continues until there are no more axes. In our case, the second axis would be at a right angle from the blue line (above). You can, at maximum, have as many PC axes as there are columns of data. However, the later axes may not explain significant portions of the underlying data, the process of rotating based upon axes of maximal variation may be able to capture the complete data set with fewer axes than the total set. This is where a technique like this may be helpful in reducing the dimensionality of the data as well as finding the ‘big trends’ that may exist in the data set. To perform this rotation on the iris data, we use the princomp() function. Here we focus only on the numerical data (of course) as the factor is not something we can do this kind of rotation with. fit.pca &lt;- princomp(iris[,1:4], cor = TRUE) Here are the first 8 (out of 50 potential) axes for the arapat data set. summary(fit.pca) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Standard deviation 1.7083611 0.9560494 0.38308860 0.143926497 ## Proportion of Variance 0.7296245 0.2285076 0.03668922 0.005178709 ## Cumulative Proportion 0.7296245 0.9581321 0.99482129 1.000000000 This output has two important components to it. First, it shows the axes, in decreasing order of importance and how much of the total variation they describe. The first Comp.1 axis explains 72.9% of the variance, the second explains 22.8%, etc. Second, it shows the cumulative proportion of the variation explained. From the 4 axes we started with, we can explain 95.8% of the variance by using just the first two PC axes. Where this becomes meaningful for us is in how we can project our original data onto these new coordinate locations and look at the distribution to see if there are any obvious trends, partitions, gradients, etc. library(ggplot2) pred &lt;- predict(fit.pca) df &lt;- data.frame(PC1 = pred[, 1], PC2 = pred[, 2]) df$Species &lt;- iris$Species ggplot(df) + geom_point(aes(x = PC1, y = PC2, color = Species), size = 3, alpha = 0.75) We can see from the plot that the the samples are clustered in an obvious way. The designation of ‘Species’ as depicted by the color of the points, shows definite partitions between I. setosa and the remaining species (along the PC1 axis). However, projected onto the PC2 axis, there does not seem to be a partitioning of the samples differentiating the species in an obvious way. 11.4 Hierarchical Clustering In the previous section, we defined a new coordinate space for all samples in the data set. The rotation of the data was able to describe over 95% of the observed variation using only the first two PC axes. In this section, we are going to use the rotated coordinates to evaluate species-level differences using a hierarchical clustering method. Hierarchical clustering are very helpful in understanding groupings in the data, particularly if there is a ‘nesting’ structure. While there are many ways to do it, they all generally proceed as follows: 1. Define a numeric metric that measured the distances between all K groups. 2. Find the two groups that have the smallest distance and coalesce them together into a pair. 3. Assume that the coalesced pair now constitutes a single entity, estimate the numeric metric among all K-1 groups in the data set. 4. Go to #2 and repeat until you have coalesced all the groups together. Here again, it is the data that is telling us how it is structured rather than us imposing a model onto the data to see if it fits. To perform a clustering, we first need to start with a distance matrix based upon the original data. iris.dist &lt;- dist( iris[,1:4] ) We can then perform the above algorithm using the hclust() function. h &lt;- hclust( iris.dist ) plot( h ) Which shows a deep separation between groups. Lets see if we can get a bit more interpretive power out of it by changing the labels to show species: h$labels &lt;- iris$Species plot( h , cex=0.5) You may need to ‘zoom’ that one a bit to understand the labels because I reduced their font size (the cex bit in the plot statement). This is interesting but would be helpful if we could provide a bit of color to the plot to differentiate the Species. Unfortunately, we cannot just add col=Species (for reasons I don’t quite understand) to the plot but there is a way to change the h object into one that can be colored. However, you must first install a new library if you don’t already have it, which I am guessing if this is the first time you’ve done some work using trees in R you have not. Install it in the normal fashion install.packages(&quot;dendextend&quot;) Then we can use it like library( dendextend ) d &lt;- as.dendrogram( h ) labels_colors( d ) &lt;- as.numeric( iris$Species ) plot( d ) There are some interesting things to notice here. - The green setosa group seems to be nicely separated from the rest, just like we saw in the PC plot. - There are some individual samples that are a bit mixed, the I. virginica and I. versicolor samples are mixed together not forming a coherent group. These plot suggest that morphology along may be a good indicator of species differences for I. setosa but not perhaps that good for telling the differences between the I. virginica and I. versicolor samples. "],
["correlation.html", "12 Correlation 12.1 The Data 12.2 Parametric Correlations 12.3 Non-Parametric Correlation 12.4 Differences 12.5 Permutation", " 12 Correlation In this activity, we will be exploring the use of correlation and how we can get inferences about the degree of correlation between sets of data. The most important thing to remember is that correlation does not imply causation, it is only designed to determine the way variables systematically change. 12.1 The Data For this example, we will go back to our old friend, the Beer Styles data set. data &lt;- read.csv(&quot;data/Beer_Styles.csv&quot;) summary(data) ## Styles Yeast ABV_Min ABV_Max ## Altbier : 1 Ale :69 Min. :2.400 Min. : 3.200 ## Amber Kellerbier : 1 Either: 4 1st Qu.:4.200 1st Qu.: 5.475 ## American Amber Ale : 1 Lager :27 Median :4.600 Median : 6.000 ## American Barleywine: 1 Mean :4.947 Mean : 6.768 ## American Brown Ale : 1 3rd Qu.:5.500 3rd Qu.: 8.000 ## American IPA : 1 Max. :9.000 Max. :14.000 ## (Other) :94 ## IBU_Min IBU_Max SRM_Min SRM_Max ## Min. : 0.00 Min. : 8.00 Min. : 2.00 Min. : 3.00 ## 1st Qu.:15.00 1st Qu.: 25.00 1st Qu.: 3.50 1st Qu.: 7.00 ## Median :20.00 Median : 35.00 Median : 8.00 Median :17.00 ## Mean :21.97 Mean : 38.98 Mean : 9.82 Mean :17.76 ## 3rd Qu.:25.00 3rd Qu.: 45.00 3rd Qu.:14.00 3rd Qu.:22.00 ## Max. :60.00 Max. :120.00 Max. :30.00 Max. :40.00 ## ## OG_Min OG_Max FG_Min FG_Max ## Min. :1.026 Min. :1.032 Min. :0.998 Min. :1.006 ## 1st Qu.:1.040 1st Qu.:1.052 1st Qu.:1.008 1st Qu.:1.012 ## Median :1.046 Median :1.060 Median :1.010 Median :1.015 ## Mean :1.049 Mean :1.065 Mean :1.009 Mean :1.016 ## 3rd Qu.:1.056 3rd Qu.:1.075 3rd Qu.:1.010 3rd Qu.:1.018 ## Max. :1.080 Max. :1.130 Max. :1.020 Max. :1.040 ## Intuitively, we have been displaying data in the manner appropriate for correlations for some time using the scatter plot. Here is an example display using the maximum Original Gravity (a measure of how much sugar is in the pre-fermented beer—technically called wort) and the final level of alcohol (what the yeast makes by munching on the sugar). These are obviously related variables, you cannot have a high alcohol fermented liquid by starting with a low sugar wort. library( ggplot2 ) x &lt;- data$OG_Max y &lt;- data$ABV_Max df &lt;- data.frame( x, y ) ggplot( df, aes(x,y) ) + geom_point() + xlab(&quot;ABV&quot;) + ylab(&quot;OG&quot;) The extent to which these variables change together is quantified by a correlation statistic. cor(x,y) ## [1] 0.9541928 This correlation statistic, \\(\\rho\\), is bound between -1 (perfect negative correlation) and +1 (perfect positive correlation). In our example here, the correlation is \\(\\rho=\\) 0.954, suggesting that there is a positive association and it is quite strong (close to +1). Using a combination of either plot() or geom_point() and cor(), we can both display and evaluate the correlation between two variables. For more than two, we can either iteratively go through all possible pairs and plot/evaluate them individually, or we can use the GGally package to plot all pairs of variables. You may need to install the GGally package (it is not installed by default). If you get an error message when you use library(GGally) install the package from CRAN using the command install.packages(&quot;GGally&quot;). Here I plot the maximum values for each property of the styles (and change the base font size so that it shows up properly on the PDF output). library(GGally) ggpairs(data[c(4,6,8,10,12)]) + theme_bw(base_size = 8) There are three components to this graphical output. Above the diagonal is the pair-wise correlation, on the diagonal is the density (histogram) of each variable, and below the diagonal is the plot. The ggpairs() function is quite extensible. You can specify the kinds of plots to be used above, on, and below the diagonal. You can also mix and match different types of data and it will plot them accordingly. Here is an example if I include the data$Yeast column, which is a factor. ggpairs(data[c(2,4,6,8)]) + theme_bw(base_size = 8) Because it is a factor, it presents the pair-wise correlations in a slightly different way. A word of caution. You should be very careful when using this plot with a lot of data types. It takes a bit of time to create each of these graphical outputs and display them. If you make the mistake of doing too many, you might as well go get some coffee because it will take a bit of time for it to finish. 12.2 Parametric Correlations The most common kind of correlation is Pearson’s product moment statistic. It is defined as: \\[ \\rho = \\frac{\\sum_{i=1}^N(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^N(x_i-\\bar{x})^2} \\sqrt{\\sum_{i=1}^N(x_i-\\bar{x})^2}} \\] where the numerator is a measure of the covariance between \\(x\\) and \\(y\\), divided by the product of the individual variable variance estimates. The resulting parameter \\(r\\), which approximates the statistic \\(\\rho\\), relies upon assumptions of normality. To date, we’ve been using the cor() function to estimate correlations. Check out the documentation on it by running. You will see there are several options. ?cor For our example data, we can estimate the correlation as: cor(x,y) ## [1] 0.9541928 whose default is Pearson’s estimator. Simply having a correlation is not sufficient though. Is this value significantly different than zero? How do we know? To evaluate our confidence in this statistic being significantly different than zero (both negative and positive), we can use the cor.test() function. cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 31.572, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9325548 0.9690002 ## sample estimates: ## cor ## 0.9541928 Here, we have the option of setting the null hypothesis in terms of what we are testing. This comes down to evaluating what the alternative hypothesis should be. alternative=&quot;two.sided&quot; Are we evaluating both greater than or less than? alternative=&quot;less&quot; Are we only concerned about rejecting the null hypothesis if it is less alternative=&quot;greater&quot; Are we only concerned with rejections if it is greater? These alternative dictate where we determine the ‘area under the curve’ for assigning probabilities. Just like in other examples, the analysis itself in R returns on object that is a type of variable. rho &lt;- cor.test(x,y) class(rho) ## [1] &quot;htest&quot; This type can be considered a list and you can gain access to the internal components in it. names(rho) ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;estimate&quot; &quot;null.value&quot; ## [6] &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; &quot;conf.int&quot; using the normal methods rho$estimate ## cor ## 0.9541928 rho$alternative ## [1] &quot;two.sided&quot; This can come in handy when you want to add components of an analysis to some graphical output. In this example, I add the estimate and the probability of the correlation being significantly different than zero to a scatter plot. plot(x,y,xlab=&quot;The X Value&quot;, ylab=&quot;The Y Value&quot;, bty=&quot;n&quot;) r &lt;- paste( &quot;r =&quot;, format( rho$estimate, digits=4) ) p &lt;- paste( &quot;P =&quot;, format( rho$p.value, digits=4) ) msg &lt;- paste( r, p, sep=&quot;\\n&quot;) text( 1.050, 12, msg) 12.3 Non-Parametric Correlation In addition to parametric approaches, we have a host of non-parametric approaches that we can use to evaluate the correlation between variables. The most common one is Spearman’s Rank Correlation. The idea here is that if your data are not conforming to assumptions of normality, you can summarize your data by ranking them instead and derive the correlation based upon the ranks of your data instead of on the raw data itself. Here is an example where I rank both \\(x\\) and \\(y\\) in the data.frame. df &lt;- df[ order(df$x),] df$X_Rank &lt;- 1:nrow(df) df &lt;- df[ order(df$y),] df$Y_Rank &lt;- 1:nrow(df) df[1:10,] ## x y X_Rank Y_Rank ## 43 1.035 3.2 4 1 ## 96 1.032 3.3 2 2 ## 15 1.034 3.6 3 3 ## 95 1.038 3.6 6 4 ## 76 1.032 3.8 1 5 ## 40 1.038 3.8 5 6 ## 34 1.039 3.8 7 7 ## 44 1.040 3.9 9 8 ## 8 1.044 4.1 11 9 ## 1 1.040 4.2 8 10 You can see that the smallest values in \\(x\\) are not the smallest values in \\(y\\) but they are pretty close. in fact, the estimator for this statistic is identical to that for Pearson’s \\(\\rho\\) except that instead of using the raw data, we use the rank. \\[ \\rho_{Spearman} = \\frac{\\sum_{i=1}^N(Rx_i-\\bar{Rx})(Ry_i-\\bar{Ry})}{\\sqrt{\\sum_{i=1}^N(Rx_i-\\bar{Rx})^2} \\sqrt{\\sum_{i=1}^N(Rx_i-\\bar{Rx})^2}} \\] cor(x,y,method = &quot;spearman&quot;) ## [1] 0.9493338 If there are no ties in the values (e.g., how we assign a rank to a set of \\(x\\) or \\(y\\) values that are identical), then \\[ \\rho_{Spearman} = \\frac{6\\sum_{i=1}^N d_i^2}{N(N^2-1)} \\] where \\(d_i = Rx_i - Ry_i\\) (e.g., the difference in the rank values). If there are ties in the raw data (as we have in ours) then fractional ranks are given to all the values that have the same observation. Look at the output above, we see that for the values of \\(y\\) we have two that both have an assigned value of 3.6, the third and fourth observation. To assigned tied ranks we would assign both of them a rank of 3.5. We would assign a rank of 6 for those whose values are 3.8, etc. If there are ties, we are warned about this when we test significance cor.test( x, y, method=&quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: x and y ## S = 8443.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.9493338 You could get around it a bit if you either increase the specificity of your measurements or throw away data. In our case, I can do neither so it requires me to take a hit in the way in which the probability is estimated. No ties allows some simplifying assumptions to be me, whereas having them does prevents us from using them. 12.4 Differences So what is the difference? Why use one over approach over the other? In general, if they give similar responses to the same data cor(x,y) ## [1] 0.9541928 cor(x,y,method=&quot;spearman&quot;) ## [1] 0.9493338 Here the difference between them is 0.004859 with the parametric one giving a slightly larger correlation. There is some loss of power going from observations to ranked observations. This is not always the case though. Consider the data2 below. a &lt;- seq(-1.6, 1.6, by=0.05) b &lt;- sin(a)*20 + rnorm(length(a),sd=1) plot(a,b) As we should expect, these data are probably not normally distributed. qqnorm(b) qqline(b,col=&quot;red&quot;) with deviations on the tails. shapiro.test(b) ## ## Shapiro-Wilk normality test ## ## data: b ## W = 0.90311, p-value = 9.309e-05 But if we look at the correlations cor(a,b) ## [1] 0.9900828 cor(a,b,method=&quot;spearman&quot;) ## [1] 0.9927448 in this case, the rank correlation is higher (0.0026619, not a lot but enough to prove the point that parametric approximations are not always producing higher estimates. 12.5 Permutation Before we finish, I want to make a diversion into permutation. A lot of what we do in Biology may be done on data whose underlying distributional assumptions (normality, etc.) are generally unknown. Many times, we can make assumptions (or transform our data) in such a way as to approximate the underlying assumptions. However, that is not always the case. In the last decade, we’ve relied upon permutation as a method for evaluating probabilities associated with correlations (and a whole host of other statistics) and have opened a large door onto a lot of new analyses. The main idea behind permutation is that you have a null hypothesis, say: \\[ H_O: \\rho=0 \\] That is, you are expecting that the correlation between \\(x\\) and \\(y\\) is non-existent. If this is TRUE then the value of \\(\\rho\\) you estimate should be just as big if you took one of your data sets, say \\(y\\), and permuted it. If the NULL hypothesis is TRUE any permutation of \\(y\\) should produce values of \\(\\hat{\\rho}\\) that are as large as you got in the original analysis. So, one way to test the amount of support we have in \\(H_O\\) is to do just that. Say we want to evaluate the significance associated with rejecting the null hypothesis of no correlation when we observed \\(\\rho =\\) 0.9542. We can create a large number (say 999 permuted values) and look at the distribution of \\(\\hat{\\rho}\\) estimates. # Make place to store permuted and observed values r_null &lt;- rep(NA,999) # Assign observed value r_null[1000] &lt;- cor( x, y ) # Make 1000 permutation, each time assigning new rho for( i in 1:999) { yp &lt;- sample( y, size = length(x), replace = FALSE) r_null[i] &lt;- cor( x, yp ) } If we look at these values, we see that our observed correlation is way out on the right end of the NULL distribution. df &lt;- data.frame( rho=r_null ) observation &lt;- c( rep( &quot;Permuted&quot;,999), &quot;Observed&quot; ) df$Observation &lt;- factor( observation ) ggplot( df, aes(x=rho,fill=Observation)) + geom_histogram(bins=50) We can evaluate the Probability of the NULL being correct as the fraction of all those values which are as large or larger than the observed one. P &lt;- sum( r_null &gt;= cor(x,y) ) / 1000 P ## [1] 0.001 Which in this case is 1/1000! The observed value is the largest. You will see more and more statistical approaches that use this ‘trick’ (it is really a trick and drives many statisticians a bit crazy) in Biology and Ecology because the underlying distributions are largely unknown. It does become tricky though, when you have more complicated models and there is currently a lot of research being conducted on models that have nesting (e.g., populations within regions, etc.) or other designs more complicated than the most simple ones. I am making this data up, it uses a random number generator and your data is probably not going to produce the same identical correlation but it will be close.↩ "],
["linear-regression.html", "13 Linear Regression 13.1 The Data 13.2 Least Squares Linear Regression 13.3 Fitting a Linear Model 13.4 Multiple Regression", " 13 Linear Regression In this activity, we explore the concepts of model development using regression. Here we show how to create a model, extract parameters from the model, and evaluate the appropriateness of the model. 13.1 The Data For this exercise, we will play with some different data. In this case, we will use some data that is in the MASS package describing 1993 vehicles. library(MASS) names(Cars93) ## [1] &quot;Manufacturer&quot; &quot;Model&quot; &quot;Type&quot; ## [4] &quot;Min.Price&quot; &quot;Price&quot; &quot;Max.Price&quot; ## [7] &quot;MPG.city&quot; &quot;MPG.highway&quot; &quot;AirBags&quot; ## [10] &quot;DriveTrain&quot; &quot;Cylinders&quot; &quot;EngineSize&quot; ## [13] &quot;Horsepower&quot; &quot;RPM&quot; &quot;Rev.per.mile&quot; ## [16] &quot;Man.trans.avail&quot; &quot;Fuel.tank.capacity&quot; &quot;Passengers&quot; ## [19] &quot;Length&quot; &quot;Wheelbase&quot; &quot;Width&quot; ## [22] &quot;Turn.circle&quot; &quot;Rear.seat.room&quot; &quot;Luggage.room&quot; ## [25] &quot;Weight&quot; &quot;Origin&quot; &quot;Make&quot; These data were taken from Car and Driver in 1993 and contain information on 93 different models that were produced that year. For our 13.2 Least Squares Linear Regression If both your predictor and response data are numeric and you are attempting to create a mathematical representation of one variable in terms of the other, a regression approach is appropriate. The simplest model, containing one predictor and one response variable can be written as: \\[ y_{ij} = \\beta_0 + \\beta_1x_i + \\epsilon_j \\] Where \\(y_{ij}\\) is the observed value, \\(\\beta_0\\) is the intercept term (where the line crosses the y-axis), \\(\\beta_1\\) is the slope coefficient on the \\(x_1\\) variable, and \\(\\epsilon_j\\) is the error term–what is not explained by the model. Least Squares Linear Regression is perhaps the most commonly used method to estimate a regression model and it is the one that is implemented in the functions below. To estimate the model, it follows the basic steps (illustrated in the figure below). The regression model will fit a line consisting of points we will call \\(\\hat{y}\\). This line will go through the mean of both the x- and y- data points (\\(\\bar{x}\\) and \\(\\bar{y}\\) respectively). The slope of the line, anchored at \\((\\bar{x}, \\bar{y})\\), will be determined by finding a regression coefficient, \\(\\beta_1\\), that minimizes the sum of the distances between each observed value and its corresponding fitted value (e.g., select a \\(\\beta_1\\) that makes the smallest \\(\\sum (y_i - \\hat{y}_i)^2\\)). Estimate \\(\\beta_0\\) the equation above using \\(\\beta_1\\) while inserting \\(\\bar{y}\\) and \\(\\bar{x}\\) for the response and predictor variables. Least squares regression fitting. It is a rather simple approach and one that is expandable across a broad range of conditions. However, there are some assumptions that need to be met when using a least squares linear regression. These include: A linear relationship. The terms we used above are specifically linear (e.g., there are no higher order exponents on any of the values). If you suspect that there is a non-linear relationship between the variables, you can explore models that use modifications of the original data (e.g., \\(\\sqrt{y}\\), \\(x^3\\), etc.). Normality of the data. As we saw earlier in the semester, we need to make certain assumptions about the underlying form of the data we are working with. In this case, we will be assuming that the data we are working with conform to normal expectations. You should test that a priori and we will see a bit later how it can be examined in the variation in the response not described by the model (e.g., the \\(\\epsilon\\) in the equation above). Homoscedasticity. Scedasticity measures the variation in a variable throughout its range. It is important for us to look to make sure that the variation in our response variable is roughly uniform across its range (homoscedastic) as opposed to having a lot of variation on one end and little on the other (a condition of heteroscedasticity). No autocorrelation. For ecological data, this is an important component. If we are collecting data, the spatial proximity of where we collect the data should not contribute to similarity in the response variable. No colinearity among predictors. If you are fitting a model with more than one predictor variable, they should not be co-linear. What does this mean? It means that you should not use predictor variables that are highly correlated. How high? Well, one thought is that if \\(1-R^2 &lt; 0.2\\) you may need to be concerned (where \\(R^2\\) is the fraction of the variation you can explain in one predictor by another using a linear regression approximation). 13.3 Fitting a Linear Model To fit a model in R, we use the function lm() (literally short of ‘linear model’) and specify the formula we are trying to fit. In the following example, we will be looking to see the extent to which we can predict horsepower as a function of engine size. library(ggplot2) p &lt;- ggplot( Cars93, aes(x=EngineSize, y=Horsepower) ) + geom_point() p + geom_text( aes(label=Make, y=Horsepower+10), data=Cars93[57,]) Figure 13.1: Horsepower as a function of engine size (in Liters) for 93 different vehicles available in 1993. The Mazda RX-7 is labeled as it is a rotary engine, an entirely different kind of engine than the rest. As such, we specify the formula Horsepower ~ EngineSize which means, literally, that Horsepower is a function of EngineSize. fit &lt;- lm( Horsepower ~ EngineSize, data=Cars93) fit ## ## Call: ## lm(formula = Horsepower ~ EngineSize, data = Cars93) ## ## Coefficients: ## (Intercept) EngineSize ## 45.22 36.96 The interesting terms here are the intercept (\\(\\beta_0\\)) and the coefficient (the \\(\\beta_1\\)) for the model fitting Horsepower to EngineSize. As we saw previously in the case of a cor.test() examples, the thing that is returned from a lm() function call is a specific type of R object (it is really just a list), class(fit) ## [1] &quot;lm&quot; one with specific names and terms contained within it. names(fit) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; As before, we can use this to make a plot of the model and the data. plot( Horsepower ~ EngineSize, data=Cars93, xlab =&quot;Engine Size (Liters)&quot;, ylab=&quot;Horsepower&quot; ) abline(a=fit$coefficients[1], b=fit$coefficients[2], col=&quot;red&quot;) b0 &lt;- format( fit$coefficients[1], digits=3) b1 &lt;- format( fit$coefficients[2], digits=3) msg &lt;- paste( &quot;y = &quot;,b0, &quot; + &quot;, b1, &quot;x&quot;, sep=&quot;&quot;) text( 4.5,100, msg) How does the model look? Do you think it is a good fit? Lets look at some output of the lm object. summary(fit) ## ## Call: ## lm(formula = Horsepower ~ EngineSize, data = Cars93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -75.910 -19.664 -9.146 15.247 161.728 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.219 10.312 4.385 3.11e-05 *** ## EngineSize 36.963 3.605 10.253 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.87 on 91 degrees of freedom ## Multiple R-squared: 0.536, Adjusted R-squared: 0.5309 ## F-statistic: 105.1 on 1 and 91 DF, p-value: &lt; 2.2e-16 We see the terms, an estimate of their magnitude, and associated probability of keeping them in the model (assuming they are not zero). In the middle of the output, we see the R-squared values. \\[ R^2 = \\frac{SS_{Model}}{SS_{Total}} \\] which tells us what fraction of the variation in the response variable is actually explained by the model. A higher value for \\(R^2\\) means that we are explaining more of the overall variation, whereas a smaller value means less of the variance is being explained. There is a caveat here, the magnitude of \\(R^2\\) will increase as we add more predictor variables. To get around this, we can look at an adjusted-\\(R^2\\), one that is corrected by the number of terms we have in the model. The formula for that is: \\[ R^2_{Adj} = R^2 - (1-R^2)\\frac{p}{N-p-1} \\] where \\(R^2\\) is as above, \\(p\\) is the number of terms in the model, and \\(N\\) is the number of observations. In the lower part of the output, we see the estimated \\(F\\) statistic, the degrees of freedom, and the associated P-Value. The \\(F\\) statistic is the test statistic for the model and is defined as the ratio of the variation explained by the model to that of the underlying data. The more variation explained, the larger the \\(F\\) statistic and the less likely that the model should be rejected. For a look at the classic ANOVA table, we see the degrees of freedom, the Sums of Squares, the Mean Squares and the estimate \\(F\\) statistic. anova(fit) ## Analysis of Variance Table ## ## Response: Horsepower ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## EngineSize 1 135267 135267 105.12 &lt; 2.2e-16 *** ## Residuals 91 117097 1287 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The degrees of freedom are assigned as follows: The total degrees of freedom is \\(N-1\\). You allocate a degree of freedom for each predictor variable. The residual degrees of freedom (e.g. that which we did not explain) is the the rest (e.g., \\(N - p - 1\\)). The terms for the sums of squares come from the underlying data in the following way. The total sums of squared deviations are defined as: \\[ SS_{Total} = \\sum_{i=1}^N (y_i - \\bar{y})^2 \\] which are composed of the model sums of squares (the variation in the response explained by the model) \\[ SS_{Model} = \\sum_{i=1}^N (\\hat{y}_i - \\bar{y})^2 \\] and the residual (sometimes called error) variation \\[ SS_{Residual} = \\sum{i=1}^N (y - \\hat{y}_i)^2 \\] These values denote the fit of the underlying model to the data, however, they are influenced by the number of samples that were collected. To estimate the mean deviation in the sums of squares, a set of parameters called the Mean Squares, you divide each of the sums of squares by its degrees of freedom. \\[\\begin{align*} MS_{Model} &amp;= \\frac{SS_{Model}}{df_{Model}} \\\\ MS_{Residual} &amp;= \\frac{SS_{Residual}}{df_{Residual}} \\\\ MS_{Total} &amp;= \\frac{SS_{Total}}{df_{Total}} \\end{align*}\\] The \\(MS_{Residual}\\) term is our best estimator of the underlying variation in the model, whereas the \\(MS_{Model}\\) is the variation associated with fitting the model. The ratio of these two terms create the test statistic, \\(F\\). \\[ F = \\frac{MS_{Model}}{MS_{Residual}} \\] This test-statistic, a ratio of variances, has a well known distribution IF the underlying data are normal. As this is well characterized, we can look up the probability of observing a value of this statistic as large or larger than that observed. To do this, we must know the degrees of freedom—more \\(df\\) means larger \\(F\\) and we need to take this into account. The expected value of \\(F\\) at \\(x\\) is defined as: \\[ F_{x,df_1,df_2} = \\frac{\\sqrt{\\frac{(df_1x)^{df_1}df_2^{df_2}}{(df_1x+df_2)^{df_1+df_2}}}}{x\\mathbf{B}\\left(\\frac{df_1}{2},\\frac{df2}{2}\\right)} \\] where \\(df_1\\) and \\(df_2\\) are the model and error degrees of freedom and \\(\\mathbf{B}\\) is the beta function. So the model ‘looks’ like it is one that explains a lot of the variation, (\\(R^2 =\\) 0.5309), the amount of which appears to be of a magnitude that suggests the null hypothesis, \\(H_O: \\beta_1 = 0\\) is not true (e.g., \\(P &lt; 2.2e-16\\)). But is it a model that fits the assumptions and is well behaved? We can absolutely have significant models that whose underlying data are not consistent with the assumptions. Fortunately, the lm() object has some built-in plotting options that help us to diagnose the appropriateness of the model. If you type plot(fit) you will be led through a series of plots depicted in the next figure. par(mfrow=c(2,2)) plot(fit, which=1) plot(fit, which=2) plot(fit, which=3) plot(fit, which=5) These depict the following items. Residuals vs. Fitted - This gives an indication of what is not explained by the model. Here if there are general trends (e.g., the residuals show systematic patterns of increase, decrease or non-linearity) these depict variance that is not explained by the underlying model. Residuals QQPlot - The variation that is not explained should also be normal, if it deviates from normality this suggests that the assumptions of the error terms may not be met in the underlying model. Scale-Location - A measure that removes any skew in the expected values. Leverage - A measure of the amount of differential influence that individual points may have in the estimation of the You must always check these values for validity in the underlying model. 13.4 Multiple Regression What if we have several potential predictor variables that we may want to put into a model? How can we determine which should be added to the model and which should not? In this example, I’m going to take vehicle weight and see if it is predicted by fit.1 &lt;- lm( Weight ~ Length, data=Cars93) fit.2 &lt;- lm( Weight ~ Width, data=Cars93) fit.3 &lt;- lm( Weight ~ EngineSize, data=Cars93) fit.4 &lt;- lm( Weight ~ Fuel.tank.capacity, data=Cars93) fit.5 &lt;- lm( Weight ~ Length + Width, data=Cars93) fit.6 &lt;- lm( Weight ~ Width + Fuel.tank.capacity, data=Cars93) fit.7 &lt;- lm( Weight ~ Length + EngineSize, data=Cars93) fit.8 &lt;- lm( Weight ~ Length + Fuel.tank.capacity, data=Cars93) So how do we determine which of these model is better? Look at the output from each, which do you think? One way we could evaluate them is to look at the a R2 &lt;- c( summary(fit.1)$r.squared, summary(fit.2)$r.squared, summary(fit.3)$r.squared, summary(fit.4)$r.squared, summary(fit.5)$r.squared, summary(fit.6)$r.squared, summary(fit.7)$r.squared, summary(fit.8)$r.squared) names(R2) &lt;- paste(&quot;fit&quot;,1:8, sep=&quot;.&quot;) R2 ## fit.1 fit.2 fit.3 fit.4 fit.5 fit.6 fit.7 ## 0.6500782 0.7655560 0.7141523 0.7992683 0.7888728 0.8707672 0.7693030 ## fit.8 ## 0.8675266 Another way to evaluate different models is through the use of a measure such as AIC (Akaike’s Information Criteria) or relatives. The notion of these parameters is that there needs to be a price paid for adding additional terms to a model. It is possible for one to add additional predictor variables and eventually inflate the observed \\(R^2\\) for the model. You can even add random variables and see the same thing, they will incrementally account for small amounts of variation in the response variable. AIC is defined as: \\[ AIC = 2k - 2\\log(L) \\] where \\(k\\) is the number of terms in the model and \\(L\\) is an estimate of the log Likelihood estimator of the model. aic.vals &lt;- c( AIC(fit.1), AIC(fit.2), AIC(fit.3), AIC(fit.4), AIC(fit.5), AIC(fit.6), AIC(fit.7), AIC(fit.8) ) names(aic.vals) &lt;- names(R2) aic.vals ## fit.1 fit.2 fit.3 fit.4 fit.5 fit.6 fit.7 fit.8 ## 1357.933 1320.687 1339.124 1306.249 1312.945 1267.296 1321.189 1269.600 The better models are those with the smallest AIC values. They may be positive or negative but the smallest ones are considered to be models that are better fit. How much of a difference is considered smaller? Well, the general approach is to estimate \\(\\delta_{AIC}\\) as the difference in magnitude of the alternative AIC values from the smallest one dAIC &lt;- (aic.vals - min( aic.vals )) dAIC ## fit.1 fit.2 fit.3 fit.4 fit.5 fit.6 fit.7 ## 90.636733 53.390921 71.827437 38.952887 45.648586 0.000000 53.892522 ## fit.8 ## 2.303289 General consensus is that \\(\\delta_{AIC}\\) values that are between 0-2 are small enough that the models are indistinguishable—they should all be considered as equally informative. Values of \\(\\delta_{AIC}\\) between 3-5 suggest that the models are pretty close and you may want to explore them further. Values greater than 5 suggest that those models are not as good as the one with the smallest AIC. Here is a more readable output from these model tests. Models &lt;- as.character( c( formula(fit.1$terms), formula(fit.2$terms), formula(fit.3$terms), formula(fit.4$terms), formula(fit.5$terms), formula(fit.6$terms), formula(fit.7$terms), formula(fit.8$terms))) df &lt;- data.frame( Model=names(dAIC), Terms=Models, R2=R2, AIC=aic.vals, Delta.AIC=dAIC) knitr::kable(df, row.names = FALSE,digits = 3) Model Terms R2 AIC Delta.AIC fit.1 Weight ~ Length 0.650 1357.933 90.637 fit.2 Weight ~ Width 0.766 1320.687 53.391 fit.3 Weight ~ EngineSize 0.714 1339.124 71.827 fit.4 Weight ~ Fuel.tank.capacity 0.799 1306.249 38.953 fit.5 Weight ~ Length + Width 0.789 1312.945 45.649 fit.6 Weight ~ Width + Fuel.tank.capacity 0.871 1267.296 0.000 fit.7 Weight ~ Length + EngineSize 0.769 1321.189 53.893 fit.8 Weight ~ Length + Fuel.tank.capacity 0.868 1269.600 2.303 It appears that both models fit.6 and fit.8, the two models with the size of the fuel tank, do a pretty reasonable job of describing the weight of the cars. Strictly speaking fit.6 is probably the most explanatory of the models with fit.8 being pretty close. "],
["analysis-of-variance.html", "14 Analysis of Variance 14.1 One Sample Hypotheses 14.2 Two Sample Hypotheses 14.3 Many Sample Hypotheses", " 14 Analysis of Variance Here we explore methods applicable to testing hypotheses about specific parameters, as equal to a specified value (a \\(t\\)-test), equal between two samples (paired \\(t\\)-tests), and equal among more than two groups (analysis of variance). In this and the next section, we will examine how to tell the difference between measures of central tendency. This is applied to a single set of data, pairs of data sets, and a data set with many different groups. As we increase the complexity of these hypotheses, we will move through a series of statistical tests, starting with a one-sample \\(t\\)-test, a paired \\(t\\)-test, and then to the general analysis of variance (ANVOA) approaches. All of these methods evaluate the equality of mean values. 14.1 One Sample Hypotheses At the most basic level, we can take a set of data and test to see if the mean of those values are equated to some particular value, \\(H_O: \\mu = x\\) (or \\(H_O: \\mu = 0\\) in some cases). The idea here is to determine, by specifying a value for the null hypothesis, what we expect the mean value to be equal to. Going back to our idea of hypothesis testing, the null hypothesis is the thing we are trying to disprove (with some level of statistical confidence) and in doing so we need to define a test statistic that we have an idea about its behavior. In this case, we will define Student’s \\(t\\)-test statistic as: \\[ t =\\frac{\\bar{x}-\\mu}{s_{\\bar{x}}} \\] where \\(\\bar{x}\\) is the observed mean of the data, \\(\\mu\\) is the mean value specified under the null hypothesis, and \\(s_{\\bar{x}}\\) is the standard deviation of the data. The value of the \\(t\\)-statistic can be defined based upon the sample size (e.g., the degrees of freedom, \\(df\\)). Here is what the probability density function looks like for \\(df = (1,3,\\infty)\\). library( ggplot2 ) x &lt;- seq(-5,5,by=0.02) d &lt;- data.frame( t=c(x,x,x), f=c(dt(x,df=1), dt(x,df=3), dt(x,df=Inf)), df=rep(c(&quot;1&quot;,&quot;3&quot;,&quot;Inf&quot;),each=length(x))) ggplot( d, aes(x=t,y=f,color=df)) + geom_line() When \\(df=\\infty\\) then \\(PDF(t) = Normal\\). As such, we do not need to make corrections to understand the area under the curve, we can just use the normal probability density function. In fact, when \\(df=\\infty\\) then \\(t_{\\alpha,\\infty} = Z_{\\alpha} = \\sqrt{\\chi^2_{\\alpha,df=1}}\\)! The take home message here is that all your statistics become much easier when \\(N=\\infty\\), so go collect some more data! For \\(df &lt; \\infty\\) (all the cases we will be dealing with), we will use the approximation defined by the \\(t\\) distribution. If you look at the distributions above, you see that as we increase the number of samples (e.g., as \\(df\\) increases), the distribution becomes more restricted. The actual function is defined (where \\(df = v\\) for simplicity in nomenclature) as: \\[ P(t|x,v)= \\frac{ \\Gamma\\left( \\frac{v+1}{2}\\right)}{\\sqrt{v\\pi}\\Gamma\\left( \\frac{v}{2}\\right)} \\left( 1 + \\frac{x^2}{v}\\right)^{-\\frac{v+1}{2}} \\] where \\(\\Gamma\\) is the Gamma function. Not pretty! Fortunately, we have some built-in facilities in R that can make it easy for us. For a single set of data, we can use the function above to estimate a value of the \\(t\\) statistic. The probability distribution, defined by the degrees of freedom, identifies regions within which we may suspect the statistic to be abnormally large. In our case, though it is quite arbitrary, we can define either one or two regions of the distribution whose values would be extreme enough such that we would consider a significant deviation. For a two-tailed test, the distribution below illustrates this concept. If the estimated value of the \\(t\\) statistic is in either of the shaded regions, we would reject the null hypothesis of \\(H_O: \\mu = 0\\) where \\(\\alpha=0.05\\). d1 &lt;- data.frame(t=c( seq(-5,-2.064, by=0.02), -2.064, -5), f=c( dt( seq(-5,-2.064, by=0.02),df=1), 0.01224269, 0.01224269)) d2 &lt;- data.frame(t=c( seq(2.064,5,by=0.02), 5, 2.064), f=c( dt( seq( 2.064, 5, by=0.02),df=1), 0.01224269, 0.01224269)) d3 &lt;- data.frame( x=c(2.5,-2.5), y=0.02719, label=&quot;2.5%&quot;) ggplot() + geom_polygon(aes(t,f),data=d1, fill=&quot;#F8766D&quot;,alpha=0.5,color=&quot;#F8766D&quot;) + geom_polygon(aes(t,f),data=d2, fill=&quot;#F8766D&quot;,alpha=0.5,color=&quot;#F8766D&quot;) + geom_line( aes(t,f),data=d[d$df==1,], color=&quot;#F8766D&quot;) + geom_text( aes(x,y,label=label),data=d3) In R, we can use the t.test() function. I’m going to go back to the Iris data set and use that as it has three categories (the species) and many measurements on sepals and pedals. Here I separate the species into their own data.frame objects. df.se &lt;- iris[ iris$Species == &quot;setosa&quot;,] df.ve &lt;- iris[ iris$Species == &quot;versicolor&quot;,] df.vi &lt;- iris[ iris$Species == &quot;virginica&quot;,] Lets look at the Sepal.Length feature in these species and create some hypotheses about it. ggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_density(alpha=0.75) We could test the hypothesis, \\(H_O: mean(Sepal.Length)=6\\) for each of the species. fit.se &lt;- t.test(df.se$Sepal.Length, mu = 6.0) fit.se ## ## One Sample t-test ## ## data: df.se$Sepal.Length ## t = -19.94, df = 49, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 6 ## 95 percent confidence interval: ## 4.905824 5.106176 ## sample estimates: ## mean of x ## 5.006 From the output, it appears that we can reject that null hypothesis (\\(t =\\) -19.9; \\(df =\\) 49; \\(P =\\) 3.7e-25). For I. versicolor, we see that the mean does appear to be equal to 6.0 (and thus fail to reject the null hypothesis): t.test( df.ve$Sepal.Length, mu=6.0 ) ## ## One Sample t-test ## ## data: df.ve$Sepal.Length ## t = -0.87674, df = 49, p-value = 0.3849 ## alternative hypothesis: true mean is not equal to 6 ## 95 percent confidence interval: ## 5.789306 6.082694 ## sample estimates: ## mean of x ## 5.936 and for I. virginica, we find that it is significantly larger than 6.0 and again reject the null hypothesis: t.test( df.vi$Sepal.Length, mu=6.0 ) ## ## One Sample t-test ## ## data: df.vi$Sepal.Length ## t = 6.5386, df = 49, p-value = 3.441e-08 ## alternative hypothesis: true mean is not equal to 6 ## 95 percent confidence interval: ## 6.407285 6.768715 ## sample estimates: ## mean of x ## 6.588 In all the output, we are also given an estimate of the Confidence Interval around the mean. This confidence interval is determined as: \\[ \\bar{x} - t_{\\alpha, df} s_{\\bar{x}} &lt; \\mu &lt; \\bar{x} + t_{\\alpha, df} s_{\\bar{x}} \\] or the mean plus or minus standard deviation of the data times the value of the \\(t\\)-statistic for a given level of \\(\\alpha\\) and \\(df\\). 14.1.1 Data Variability There are times when reporting some confidence around a parameter is important, particularly when using tabular data as output. Species &lt;- c(&quot;Iris setosa&quot;,&quot;Iris versicolor&quot;,&quot;Iris virginia&quot;) Sepal.Length &lt;- c(mean(df.se$Sepal.Length), mean(df.ve$Sepal.Length), mean( df.vi$Sepal.Length)) Sepal.Length.SE &lt;- c(sd(df.se$Sepal.Length), sd(df.ve$Sepal.Length), sd( df.vi$Sepal.Length)) Sepal.Length.SEM &lt;- Sepal.Length.SE / sqrt(50) There are two ways we can talk about the data and it is important for you to think about what you are trying to communicate to your readers. These alternatives include: sd &lt;- paste( format(Sepal.Length,digits=2), &quot;+/-&quot;, format(Sepal.Length.SE, digits=3)) se &lt;- paste( format(Sepal.Length,digits=2), &quot;+/-&quot;, format(Sepal.Length.SEM, digits=3)) df &lt;- data.frame( Species, sd, se ) names(df) &lt;- c(&quot;Species&quot;,&quot;Mean +/- SD&quot;, &quot;Sepal Length +/- SE&quot;) knitr::kable(df,row.names = FALSE,digits = 3,align = &quot;lcc&quot;) Species Mean +/- SD Sepal Length +/- SE Iris setosa 5.0 +/- 0.352 5.0 +/- 0.0498 Iris versicolor 5.9 +/- 0.516 5.9 +/- 0.0730 Iris virginia 6.6 +/- 0.636 6.6 +/- 0.0899 The two columns of data tell us something different. The middle column tells us the mean and the standard deviation of the data. This tells us about the variability (and confidence) of the data itself. The last column is the Standard Error of the Mean (\\(\\frac{s}{\\sqrt{N}}\\)) and gives us an idea of the confidence we have about the mean estimate of the data (as opposed to the variation of the data itself). These are two different statements about the data and you need to make sure you are confident about which way you want to use to communicate to your audience. 14.2 Two Sample Hypotheses In addition to a single sample test, evaluating if the mean of a set of data is equal to some specified value, we can test the equality of two different samples. It may be the case that the average sepal length for I. versicolor is not significantly different than 6.0 whereas I. virginia is. However, this does not mean that the mean of both of these species are significantly different from each other. This is a two-sampled hypothesis, stating that \\(H_O: \\mu_X = \\mu_Y\\). Visually, these data look like: df &lt;- iris[ (iris$Species %in% c(&quot;versicolor&quot;,&quot;virginica&quot;)),] ggplot( df, aes(x=Species, y=Sepal.Length)) + geom_boxplot(notch=TRUE) which clearly overlap in their distributions but are the mean values different? This sets up the null hypothesis: \\[ H_O: \\mu_1 - \\mu_2 = 0 \\] Under this hypothesis, we can use a t-test like before but just rearranged as: \\[ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}_1-\\bar{x}_2}} \\] As before, if the difference in the numerator is small we would reject but here we need to standardize the differences in the means by a measure of the standard deviation that is based upon both sets of data. This is called a the standard error of the difference in two means (real catchy title, no?). This is defined as: \\[ s_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{ \\frac{s_1^2}{N_1}+\\frac{s_2^2}{N}} \\] To test this, we use the same approach as before but instead of defining \\(\\mu = 6.0\\) in the t.test() function, we instead give it both data sets. t.test( x=df.vi$Sepal.Length, y = df.ve$Sepal.Length ) ## ## Welch Two Sample t-test ## ## data: df.vi$Sepal.Length and df.ve$Sepal.Length ## t = 5.6292, df = 94.025, p-value = 1.866e-07 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4220269 0.8819731 ## sample estimates: ## mean of x mean of y ## 6.588 5.936 Here we get a few bits of new information from the analysis. It is obvious that we would reject the null hypothesis given the magnitude of the estimated \\(P\\)-value. The output also provides us an estimate of the mean values for each group as well as the confidence around the difference in the mean values. This confidence interval does not overlap 0.0, as it shouldn’t if we reject \\(H_O: \\mu_X = \\mu_Y\\). 14.3 Many Sample Hypotheses If we have more than two samples, we could do a bunch of paired \\(t\\)-test statistics but this is not the best idea. In fact, if we do this to our data, each time testing at a confidence level of, say, \\(\\alpha = 0.05\\), then for each time we test at \\(0.05\\) but over all pairs, we test at an overall level of \\(0.05^k\\) (where \\(k\\) is the number of tests) value. We cannot do multiple tests without penalizing ourselves in terms of the level at which we consider something significant if we are going to do all these tests. You may have heard about a Bonferroni correction—this does exactly that, it allows us to modify the \\(\\alpha\\) level we use to take into consideration the number of tests we are going to use. While this may be an acceptable way to test for the equality of several means (and it may not actually be if you ask most statisticians), there is another way that is much easier. Consider the case where we have many categories (e.g., factors in R) that we are interested in determining if the mean of all are equal. The null hypothesis for this is, \\(H_O: \\mu_1 = \\mu_2 = \\ldots = \\mu_k\\), where there are \\(k\\) different treatment levels. This is essentially what we’d want to do by doing a bunch of \\(t\\)-tests but we can use another approach that we don’t have to penalize ourselves for multiple tests. Here is how it works. In the Iris data, we can visualize the means and variation around them by using box plots. Here is an example. ggplot( iris, aes(x=Species, y=Sepal.Length)) + geom_boxplot(notch = TRUE) + ylab(&quot;Sepal Length&quot;) For us to tell if there are statistical differences among the species, we need to look at both the location of the mean values as well as the variation around them. We do this by partitioning the variation in all the data into the components within each treatment (species) and among each treatment (species) using an approach derived from the sum of squared deviations (or Sums of Squares). Formally, we can estimate the sum of squares within each of the \\(K\\) groupings as: \\[ SS_{Within} = \\sum_{i=1}^K\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right) \\] whose degrees of freedom are defined as: \\[ df_{W} = \\sum_{i=1}^K \\left( N_i - 1 \\right) = N-K \\] These parameters represent the deviation among samples within groups and the number of independent samples within these groups. We also need to partition out the variation among groups as a similarly defined Sums of Squares: \\[ SS_{Among} = \\sum_{i=1}^K N_i\\left( \\bar{x}_i - \\bar{x} \\right)^2 \\] or the deviation among the mean of each treatment compared to the overall mean of all the data. This parameter has degrees of freedom equal to \\[ df_{A} = K - 1 \\] These two parameters describe all the data and as such \\(SS_{Total} = SS_{Within} + SS_{Among}\\). Formally, we see that \\[ SS_{Total} = \\sum_{i=1}^K\\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2 \\] whose degrees of freedom are \\[ df_{T} = N - 1 \\] For each of these Sums of Squared deviations, we can standardize them using the degrees of freedom. The notion here is that with more samples, and more treatments, we will have greater \\(SS\\) values. However, if we standardize these parameters by the \\(df\\), we can come up with a standardized Mean Squared values (simplified as \\(MS = \\frac{SS}{df}\\) for each level). If we look at all these values, we can create the venerable ANOVA table with Among, Within, and Total partitions of the variation. Source df SS MS Among \\(K-1\\) \\(\\sum_{i=1}^K N_i \\left( \\bar{x}_i - \\bar{x} \\right)^2\\) \\(\\frac{SS_A}{K-1}\\) Within \\(N-K\\) \\(\\sum_{i=1}^Kn_i\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\) \\(\\frac{SS_W}{N-K}\\) Total \\(N-1\\) \\(\\sum_{i=1}^K \\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\) In R, we can evaluate the equality of means by partitioning our data as depicted above. Essentially, if at least one of our treatments means deviate significantly, then the \\(MS_A\\) will be abnormally large relative to the variation within each treatment \\(MS_W\\). This gives us a statistic, defined by the American statistician Snedekor as: \\[ F = \\frac{MS_A}{MS_W} \\] as an homage to Ronald Fisher (the F-statistic) has a pretty well understood distribution under a few conditions. This statistic has an expectation of: \\[ f(x | df_A, df_W) = \\frac{\\sqrt{\\frac{(df_Ax)^{df_A}df_W^{df_W}}{(df_Ax + df_W)^{df_W+df_A}}}}{x\\mathbf{B}\\left( \\frac{df_A}{2}, \\frac{df_W}{2} \\right)} \\] which is even more of a mess than that for the \\(t\\)-test! Luckily, we have a bit of code to do this for us. Here is an example using the Iris data. Here we test the hypothesis that the Sepal Lengths are all the same (e.g., \\(H_O: \\mu_{se} = \\mu_{ve} = \\mu_{vi}\\)) fit.aov &lt;- aov( Sepal.Length ~ Species, data=iris) fit.aov ## Call: ## aov(formula = Sepal.Length ~ Species, data = iris) ## ## Terms: ## Species Residuals ## Sum of Squares 63.21213 38.95620 ## Deg. of Freedom 2 147 ## ## Residual standard error: 0.5147894 ## Estimated effects may be unbalanced The function called here, aov() is the one that does the Analysis of Variance. It returns an object that has the necessary data we need. To estimate the ANOVA table as outlined above we ask for it as: anova(fit.aov) ## Analysis of Variance Table ## ## Response: Sepal.Length ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 63.212 31.606 119.26 &lt; 2.2e-16 *** ## Residuals 147 38.956 0.265 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 which shows that the “Species” treatment are significantly different from each other, with an \\(F\\) statistic equal to \\(F = 119.3\\), which with 2 and 147 degrees of freedom is assigned a probability equal to \\(2e^{-16}\\), a very small value! 14.3.1 Post-Hoc Tests What this analysis tells us is that at least one of the treatment means are different from the rest. What it does not tell us is which one or which subset. It could be that I. setosa is significantly smaller than both I. versitosa and I. virginia. It could be that I. virginia is significantly larger than the others, who are not different. It could also mean that they are all different. To address this, we can estimate a post hoc test, to evaluate the difference between treatment means within this model itself. One of the most common ways to evaluate the equality of treatment mean values is that defined by Tukey. The so-called “Honest Significant Differences” post hoc test is given by tuk &lt;- TukeyHSD(fit.aov) tuk ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sepal.Length ~ Species, data = iris) ## ## $Species ## diff lwr upr p adj ## versicolor-setosa 0.930 0.6862273 1.1737727 0 ## virginica-setosa 1.582 1.3382273 1.8257727 0 ## virginica-versicolor 0.652 0.4082273 0.8957727 0 which breaks down the pair-wise differences in the mean of each treatment. Here we see the magnitude of the differences in mean values, the lower and upper confidence on the differences, and the probability associated with these differences. In this example, all three comparisons are highly unlikely (e.g., \\(P\\) is very small and in this case essentially zero). As a result, we can interpret these results as suggesting that each of the three species have significantly different. If we plot these results, we see which ones are larger and which are smaller. plot( tuk ) Which shows the difference between treatment mean between all pairs of treatments. Overall, we see that the Iris species are all significantly different. "]
]
